page	left	top	code
1	82	67	26
1	82	262	by J.-F. Coeurjolly, R. Drouilhet, P. Lafaye de Micheaux
1	82	280	and J.-F. Robineau
1	96	332	  menting large sample tests and confidence in-
1	96	350	  tervals. One and two sample mean and vari-
1	96	368	  ance tests (differences and ratios) are considered.
1	96	386	  The test statistics are all expressed in the same
1	96	403	  form as the Student t-test, which facilitates their
1	96	421	  presentation in the classroom. This contribution
1	96	439	  also fills the gap of a robust (to non-normality)
1	96	457	  alternative to the chi-square single variance test
1	96	475	  for large samples, since no such procedure is im-
1	96	493	  plemented in standard statistical software.
1	82	574	It is sometimes desirable to compare two variances
1	82	592	rather than two averages. To cite a few examples
1	82	628	would like two college professors grading exams to
1	82	646	have the same variation in their grading; in order for
1	82	664	a lid to fit a container, the variation in the lid and the
1	82	682	container should be the same; a supermarket might
1	82	700	be interested in the variability of check-out times for
1	82	718	two checkers.
1	105	735	    Now usually, a first course on statistical inference
1	82	753	presents mean tests in both Gaussian and asymptoti-
1	82	771	cal frameworks (Table 1), but variance tests are often
1	82	789	presented only in the Gaussian case (Table 2).
1	115	814	     Population law      Test statistic   Law
1	82	1043	and large sample cases.
1	169	1071	               Test statistic         Law
1	481	262	   An important point to be noticed is that stu-
1	459	280	dents are usually told that mean tests are robust to
1	459	298	non-normality for large samples as indicated by the
1	459	352	could think that this also occurs for variance tests. In-
1	459	370	deed, many practitioners use the classical chi-square
1	459	388	single variance test or Fisher’s two variances test,
1	459	406	even if the Gaussian assumption fails. This could
1	459	424	lead to heavy errors, even for large samples, as
1	459	460	this situation as "catastrophic".
1	481	478	   To have a better idea of the type I error in the
1	459	496	classical single variance test, let us test for example
1	459	568	age of rejection of the null of 21.53%, thus showing
1	459	586	a type I error far greater than α. The percentage for
1	459	604	the asymptotic test (described later) is 9.05% which is
1	459	640	variance test leads to a type I error far lesser than α
1	459	658	(0.44%). Our test still behaves correctly with a type
1	459	676	I error near α (5.39%). This is mainly due to the de-
1	459	694	parture of the kurtosis of the distribution from 3 (for
1	481	748	   Note that the problem of the robustness (to de-
1	459	766	partures from normality) of tests for comparing two
1	459	784	(or more) variances has been widely treated in the
1	459	838	ences therein. These authors built specific test statis-
1	459	856	tics. Note also that in the one sample (non Gaussian)
1	459	874	case, to the best of our knowledge, no statistical tool
1	459	891	is available to compare a population variance to a ref-
1	459	909	erence value.
1	459	964	ing a large sample test statistic may be based on an
1	459	982	estimator that has an asymptotic normal distribu-
1	459	1000	tion. Suppose we wish to test a hypothesis about a
1	459	1036	a sample of size n. If we can prove some form of the
1	459	1173	then one has the basis for an approximate test.
1	82	1218	The R Journal Vol. 1/2, December 2009
1	704	1218	                                  ISSN 2073-4859
2	790	67	                                            27
2	82	593	statistics of the chi-square variance test (top) and Fisher’s ratio of variances test (bottom) in the large sample
2	82	617	Gaussian context. The parameters of the simulation are: n
2	105	684	   This approach can be used to complete Table 2 for
2	82	702	the large sample case, shown in Table 3 for the single
2	82	720	variance test only:
2	105	917	    The case of a (large sample) test for a difference in
2	82	935	scale parameters (possibly weighted by a factor ρ) is
2	82	953	also of interest as suggested by the availability of re-
2	82	971	lated procedures in R (to compute Ansari-Bradley’s
2	82	989	and Mood’s tests for example). The standard error
2	105	1048	    The point to be noted here is that this general ap-
2	82	1101	very similar to the classical t-test from a mathemat-
2	82	1119	ical point of view. Proofs, which are not very com-
2	82	1137	plicated, are provided in the report just cited. The
2	82	1155	details are not fully expounded here but lead us to
2	82	1173	propose a more complete, homogeneous teaching
2	459	684	framework, with no additional difficulty, to test var-
2	459	702	ious parameters such as the mean, the variance, and
2	459	720	the difference or ratio of means or variances (for
2	459	738	large samples). This approach also allows the direct
2	459	756	derivation of asymptotic confidence intervals. Note
2	459	791	ilar asymptotic approach, with a refinement based
2	459	809	on a variance stabilizing transformation, to obtain
2	459	827	asymptotic confidence intervals, solely for the single
2	459	845	variance and ratio of variances cases. Table 4 gives a
2	459	863	summary of the various parameters we can test and
2	459	881	the R functions we have implemented to compute
2	459	1083	Table 4: Various parameters we can test and available
2	481	1137	   These functions can be used in conjunction with
2	459	1173	example, if you want to use a sample contained in
3	82	67	28
3	241	164	                     30    0.2168  0.2733   0.1278
3	474	164	  0.2218    0.0086   0.0801
3	241	181	                      100    0.2194  0.1765    0.1307
3	474	181	  0.1442    0.0061   0.0589
3	241	199	                      500    0.2157  0.1102    0.1367
3	474	199	  0.0928    0.0051   0.0543
3	241	217	                     1000  0.2153  0.0905   0.1323
3	474	217	  0.0787    0.0040   0.0539
3	105	401	   This contribution also solves the problem of pro-
3	82	419	viding an implemented “robust” (to departure of the
3	82	437	i.i.d. large sample distribution from normality) al-
3	82	455	ternative to the chi-square single variance test for
3	82	473	large samples. Indeed, we did not find any such
3	82	491	procedure in standard statistical software and so it is
3	82	509	highly likely that practitioners would incorrectly use
3	82	527	a chi-square test on a single variance. It also provides
3	82	545	a very simple alternative to the (ratio of variances)
3	82	563	Fisher test in large samples. Some other “robust”
3	82	581	alternative procedures to the Fisher test in the case
3	82	598	of non Gaussian (not necessary large) samples are
3	82	616	implemented in R: the Bartlett test (bartlett.test),
3	82	634	the Fligner test (fligner.test) and the Levene test
3	82	652	(levene.test available in the lawstat package). R
3	82	670	also provides, through ansari.test and mood.test
3	82	688	functions, Ansari-Bradley’s and Mood’s two-sample
3	82	706	rank-based tests for a difference in scale parameters.
3	82	724	The purpose of this paper is not to compare our tests
3	82	742	to their competitors in terms of power. We neverthe-
3	82	760	less conduct two short simulation studies (limited to
3	82	778	the probability of Type I error): first for the problem
3	82	796	of testing a variance (Table 5), comparing the clas-
3	82	832	second for the problem of comparing (the differences
3	82	885	Ansari-Bradley’s test and Mood’s test. These sim-
3	82	903	ulations were based on the three distributions used
3	82	921	earlier in Figure 1. The simulations show that the
3	82	939	level α is quite correct (when n increases) for our
3	82	957	procedure in the case of testing a single variance and
3	82	975	for all three alternative tests (ours, Ansari-Bradley’s
3	82	993	and Mood’s tests) for testing two variances.
3	106	1057	   30     0.2827  0.0675      0.0478   0.0497
3	106	1075	   100    0.3083  0.0500      0.0480   0.0484
3	106	1093	   500    0.3269  0.0454      0.0484   0.0470
3	106	1111	   1000   0.3260  0.0526      0.0501   0.0515
3	482	376	   30     0.1605  0.0676      0.0477   0.0472
3	482	393	   100    0.1797  0.0537      0.0516   0.0494
3	482	411	   500    0.1911  0.0525      0.0505   0.0498
3	482	429	   1000   0.1907  0.0526      0.0503   0.0511
3	482	566	   30     0.0029  0.0652      0.0490   0.0494
3	482	583	   100    0.0021  0.0527      0.0490   0.0475
3	482	601	   500    0.0024  0.0520      0.0511   0.0511
3	482	619	   1000   0.0022  0.0539      0.0528   0.0538
3	459	796	tion asymp.test and six auxiliary ones designed to
3	459	814	compute standard errors of estimates of different pa-
3	459	832	rameters, see Table 4. The auxiliary functions will
3	459	850	not be the most useful ones for the user, except if
3	459	868	he/she wants to compute the confidence interval
3	459	886	himself/herself. The function asymp.test has been
3	459	904	written in the same spirit as the standard R functions
3	459	940	and the resulting outputs are also inspired from these
3	459	958	functions. In particular, the function asympt.test re-
3	459	976	turns an object of class "htest" (which is the general
3	459	994	class of test objects in R).
3	481	1012	   This asymp.test function has several arguments,
3	459	1030	similar to those of the t.test function, whose
3	459	1048	description can be obtained using the command
3	481	1083	   In order to illustrate this function, let us con-
3	459	1101	sider the Digitalis Investigation Group NHLBI
3	459	1155	NHLBI. Note that statistical processes such as per-
3	459	1173	mutations within treatment groups were used to
4	790	67	                                            29
4	82	125	completely anonymize the data; therefore, inferences
4	82	143	derived from the teaching dataset may not be valid.
4	105	161	   The DIG Trial was a randomized, double-blind,
4	82	179	multicenter trial with more than 300 centers in the
4	82	197	United States and Canada participating. The pur-
4	82	215	pose of the trial was to examine the safety and effi-
4	82	233	cacy of Digoxin in treating patients with congestive
4	82	251	heart failure in sinus rhythm.
4	105	269	   Diastolic BP (DIABP, mmHg) is a known risk fac-
4	82	287	tor of cardiovascular diseases. In this case, it is de-
4	82	304	sirable to compare the variability of this quantity for
4	82	322	placebo (TRTMT=0) and treatment (TRTMT=1) groups,
4	82	340	respectively.
4	82	611	Shapiro-Wilk normality test performed by the func-
4	82	629	tion shapiro.test() indicates that the two samples
4	82	647	seem to be far from the Gaussian distribution. Thus,
4	82	664	this should prevent us from using the following
4	82	682	Fisher test.
4	82	1000	Instead, let us use our package.
4	459	230	We can see that var.test, not to be used due to the
4	459	248	unlikely normality of the data, significantly shows a
4	459	265	difference in variances (at a 5% level). We don’t ob-
4	459	283	tain the same conclusion with our test.
4	459	358	both our test and the classical chi-square test to show
4	459	775	For the above generated sample x, we respectively
4	459	793	found the following p-values: 0.0398 and 0.120. In
4	459	811	this case, we can thus see that our proposition cor-
4	459	847	square single variance test.
4	459	940	This paper has introduced a new package called
4	459	976	procedures available. It is interesting firstly in the
4	459	994	fact that it provides a unified teaching framework
4	459	1012	to present classical parametric tests (based on the
4	459	1030	Central Limit Theorem). These tests are made read-
4	459	1048	ily available in R through an easy to use function
4	459	1065	called asymp.test. This function resembles t.test
4	459	1083	or var.test, so students will not be confused. Sec-
4	459	1101	ondly, it also makes available in R a robust (to non-
4	459	1119	normality) alternative to the classical chi-square sin-
4	459	1137	gle variance test. In the future, we also plan to pro-
4	459	1155	vide tools similar to the power.t.test function in the
4	459	1173	context of large samples.
5	82	67	30
5	82	160	D. G. Bonnet.   Approximate confidence interval
5	97	178	  for standard deviation of nonnormal distributions.
5	97	195	  Computational Statistics & Data Analysis, 50:775–
5	97	213	  782, 2006.
5	82	242	D. G. Bonnet. Robust confidence interval for a ratio
5	97	259	  of standard deviations. Applied Psychological Mea-
5	97	277	  surement, 30:(5) 432–439, 2006.
5	82	305	G. E. P. Box. Non-normality and tests on variances.
5	97	323	  Biometrika, 40:(3/4) 318–335, 1953.
5	82	351	G. Casella and R. L. Berger.   Statistical Inference.
5	97	369	  Duxbury Press, Belmont, California, 2nd edition,
5	97	387	  2001.
5	82	415	J.-F. Coeurjolly, R. Drouilhet, P. Lafaye de Micheaux
5	97	433	  and J.-F. Robineau. asympTest: an R package for
5	97	451	  performing parametric statistical tests and confi-
5	97	469	  dence intervals based on the central limit theorem.
5	82	533	W. J. Conover, M. E. Johnson and M. M. Johnson.
5	97	551	  A comparative study of tests for homogeneity of
5	97	569	  variances with applications to the outer continen-
5	97	587	  tal shelf bidding data. Technometrics, 23:(4) 351–
5	97	605	  361, 1981.
5	82	633	R. Davidson and J. G. MacKinnon. Graphical meth-
5	97	651	  ods for investigating the size and power of hypoth-
5	97	669	  esis tests. The Manchester School, 66:(1) 1–26, 1998.
5	82	697	S. Dean and B. Illowsky. F Distribution and ANOVA:
5	82	761	T. Ferguson. A course in large sample theory. Chap-
5	97	778	  man and Hall, 1996.
5	82	806	R. A. Fisher. The use of multiple measurements in
5	97	824	  taxonomic problems. Annals of Eugenics, 7:(Part II)
5	97	842	  179–188, 1935.
5	459	125	R. G. Miller. Beyond ANOVA, Basics of Applied Statis-
5	474	143	  tics. Texts in Statistical Science Series. Chapman &
5	474	161	  Hall/CRC, 1997.
5	459	195	C. Ozgur and S. E. Strasser. A study of the statisti-
5	474	213	  cal inference criteria: Can we agree on when to use
5	474	231	  z versus t? Decision Sciences Journal of Innovative
5	474	249	  Education, 2:(2) 177–192, 2004.
5	459	283	G. Pan. On a Levene type test for equality of two
5	474	301	  variances. Journal of Statistical Computation and Sim-
5	474	319	  ulation, 63:59–71, 1999.
5	459	353	M. L. Tiku and A. Akkaya. Robust Estimation and Hy-
5	474	371	  pothesis Testing. New Age International (P) Ltd,
5	474	389	  New Delhi, 2004.
5	459	444	Jean-François CoeurJolly
5	459	462	L. Jean Kuntzmann, Grenoble University, France
5	459	535	Rémy Drouilhet
5	459	553	L. Jean Kuntzmann
5	459	571	Grenoble University
5	459	589	France
5	459	662	Pierre Lafaye de Micheaux
5	459	680	Département de Mathématiques et Statistique
5	459	698	Université de Montréal
5	459	716	Canada
5	459	788	Jean-François Robineau
5	459	806	CQLS
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">

<pdf2xml producer="poppler" version="0.30.0">
<page number="1" position="absolute" top="0" left="0" height="1262" width="892">
	<fontspec id="0" size="12" family="Times" color="#000000"/>
	<fontspec id="1" size="9" family="Times" color="#000000"/>
	<fontspec id="2" size="35" family="Times" color="#000000"/>
	<fontspec id="3" size="12" family="Times" color="#000000"/>
	<fontspec id="4" size="19" family="Times" color="#000000"/>
	<fontspec id="5" size="12" family="Times" color="#00004c"/>
<text top="67" left="82" width="15" height="15" font="0">30</text>
<text top="67" left="562" width="11" height="15" font="0">C</text>
<text top="69" left="573" width="86" height="12" font="1">ONTRIBUTED</text>
<text top="67" left="664" width="10" height="15" font="0">R</text>
<text top="69" left="674" width="61" height="12" font="1">ESEARCH</text>
<text top="67" left="740" width="12" height="15" font="0">A</text>
<text top="69" left="752" width="53" height="12" font="1">RTICLES</text>
<text top="121" left="82" width="677" height="36" font="2"><b>neuralnet: Training of Neural Networks</b></text>
<text top="173" left="82" width="229" height="15" font="0">by Frauke Günther and Stefan Fritsch</text>
<text top="208" left="96" width="57" height="15" font="3"><b>Abstract</b></text>
<text top="208" left="165" width="250" height="15" font="0">Artificial neural networks are applied</text>
<text top="226" left="96" width="319" height="15" font="0">in many situations. <b>neuralnet </b>is built to train</text>
<text top="244" left="96" width="319" height="15" font="0">multi-layer perceptrons in the context of regres-</text>
<text top="262" left="96" width="319" height="15" font="0">sion analyses, i.e. to approximate functional rela-</text>
<text top="280" left="96" width="319" height="15" font="0">tionships between covariates and response vari-</text>
<text top="298" left="96" width="319" height="15" font="0">ables. Thus, neural networks are used as exten-</text>
<text top="316" left="96" width="229" height="15" font="0">sions of generalized linear models.</text>
<text top="334" left="96" width="66" height="15" font="3"><b>neuralnet</b></text>
<text top="333" left="167" width="248" height="15" font="0">is a very flexible package. The back-</text>
<text top="351" left="96" width="319" height="15" font="0">propagation algorithm and three versions of re-</text>
<text top="369" left="96" width="319" height="15" font="0">silient backpropagation are implemented and it</text>
<text top="387" left="96" width="319" height="15" font="0">provides a custom-choice of activation and er-</text>
<text top="405" left="96" width="319" height="15" font="0">ror function. An arbitrary number of covariates</text>
<text top="423" left="96" width="319" height="15" font="0">and response variables as well as of hidden lay-</text>
<text top="441" left="96" width="217" height="15" font="0">ers can theoretically be included.</text>
<text top="459" left="96" width="319" height="15" font="0">The paper gives a brief introduction to multi-</text>
<text top="477" left="96" width="319" height="15" font="0">layer perceptrons and resilient backpropagation</text>
<text top="495" left="96" width="319" height="15" font="0">and demonstrates the application of <b>neuralnet</b></text>
<text top="513" left="96" width="319" height="15" font="0">using the data set infert, which is contained in</text>
<text top="531" left="96" width="120" height="15" font="0">the R distribution.</text>
<text top="576" left="82" width="124" height="21" font="4"><b>Introduction</b></text>
<text top="616" left="82" width="346" height="15" font="0">In many situations, the functional relationship be-</text>
<text top="634" left="82" width="346" height="15" font="0">tween covariates (also known as input variables) and</text>
<text top="652" left="82" width="346" height="15" font="0">response variables (also known as output variables)</text>
<text top="670" left="82" width="346" height="15" font="0">is of great interest. For instance when modeling com-</text>
<text top="688" left="82" width="346" height="15" font="0">plex diseases, potential risk factors and their effects</text>
<text top="706" left="82" width="346" height="15" font="0">on the disease are investigated to identify risk fac-</text>
<text top="724" left="82" width="346" height="15" font="0">tors that can be used to develop prevention or inter-</text>
<text top="742" left="82" width="346" height="15" font="0">vention strategies. Artificial neural networks can be</text>
<text top="760" left="82" width="346" height="15" font="0">applied to approximate any complex functional re-</text>
<text top="778" left="82" width="346" height="15" font="0">lationship. Unlike generalized linear models (GLM,</text>
<text top="796" left="82" width="194" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">McCullagh and Nelder, 1983</a></text>
<text top="796" left="276" width="153" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">), </a>it is not necessary to</text>
<text top="814" left="82" width="346" height="15" font="0">prespecify the type of relationship between covari-</text>
<text top="832" left="82" width="346" height="15" font="0">ates and response variables as for instance as linear</text>
<text top="850" left="82" width="346" height="15" font="0">combination. This makes artificial neural networks a</text>
<text top="867" left="82" width="346" height="15" font="0">valuable statistical tool. They are in particular direct</text>
<text top="885" left="82" width="346" height="15" font="0">extensions of GLMs and can be applied in a similar</text>
<text top="903" left="82" width="346" height="15" font="0">manner. Observed data are used to train the neural</text>
<text top="921" left="82" width="346" height="15" font="0">network and the neural network learns an approxi-</text>
<text top="939" left="82" width="346" height="15" font="0">mation of the relationship by iteratively adapting its</text>
<text top="957" left="82" width="78" height="15" font="0">parameters.</text>
<text top="976" left="105" width="177" height="15" font="0">The package <b>neuralnet </b><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">(</a></text>
<text top="976" left="282" width="147" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Fritsch and Günther,</a></text>
<text top="994" left="82" width="30" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">2008</a></text>
<text top="994" left="112" width="317" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">) </a>contains a very flexible function to train feed-</text>
<text top="1012" left="82" width="346" height="15" font="0">forward neural networks, i.e. to approximate a func-</text>
<text top="1030" left="82" width="346" height="15" font="0">tional relationship in the above situation. It can the-</text>
<text top="1048" left="82" width="346" height="15" font="0">oretically handle an arbitrary number of covariates</text>
<text top="1065" left="82" width="346" height="15" font="0">and response variables as well as of hidden layers</text>
<text top="1083" left="82" width="346" height="15" font="0">and hidden neurons even though the computational</text>
<text top="1101" left="82" width="346" height="15" font="0">costs can increase exponentially with higher order of</text>
<text top="1119" left="82" width="346" height="15" font="0">complexity. This can cause an early stop of the iter-</text>
<text top="1137" left="82" width="346" height="15" font="0">ation process since the maximum of iteration steps,</text>
<text top="1155" left="82" width="346" height="15" font="0">which can be defined by the user, is reached before</text>
<text top="1173" left="82" width="346" height="15" font="0">the algorithm converges. In addition, the package</text>
<text top="173" left="459" width="346" height="15" font="0">provides functions to visualize the results or in gen-</text>
<text top="191" left="459" width="346" height="15" font="0">eral to facilitate the usage of neural networks. For</text>
<text top="209" left="459" width="346" height="15" font="0">instance, the function compute can be applied to cal-</text>
<text top="227" left="459" width="333" height="15" font="0">culate predictions for new covariate combinations.</text>
<text top="245" left="481" width="324" height="15" font="0">There are two other packages that deal with artifi-</text>
<text top="263" left="459" width="288" height="15" font="0">cial neural networks at the moment: <b>nnet </b><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">(</a></text>
<text top="263" left="747" width="59" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">Venables</a></text>
<text top="280" left="459" width="111" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">and Ripley, 2002</a></text>
<text top="280" left="570" width="110" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">) </a>and <b>AMORE </b><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">(</a></text>
<text top="280" left="679" width="117" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Limas et al., 2007</a></text>
<text top="280" left="797" width="9" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">).</a></text>
<text top="298" left="459" width="31" height="15" font="3"><b>nnet</b></text>
<text top="298" left="494" width="312" height="15" font="0">provides the opportunity to train feed-forward</text>
<text top="316" left="459" width="346" height="15" font="0">neural networks with traditional backpropagation</text>
<text top="334" left="459" width="346" height="15" font="0">and in <b>AMORE</b>, the TAO robust neural network al-</text>
<text top="352" left="459" width="346" height="15" font="0">gorithm is implemented. <b>neuralnet </b>was built to train</text>
<text top="370" left="459" width="346" height="15" font="0">neural networks in the context of regression analy-</text>
<text top="388" left="459" width="346" height="15" font="0">ses. Thus, resilient backpropagation is used since</text>
<text top="406" left="459" width="346" height="15" font="0">this algorithm is still one of the fastest algorithms</text>
<text top="424" left="459" width="141" height="15" font="0">for this purpose (e.g.</text>
<text top="424" left="604" width="151" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Schiffmann et al., 1994</a></text>
<text top="424" left="756" width="4" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">;</a></text>
<text top="424" left="764" width="41" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Rocha</a></text>
<text top="442" left="459" width="70" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">et al., 2003</a></text>
<text top="442" left="529" width="4" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">;</a></text>
<text top="442" left="537" width="162" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Kumar and Zhang, 2006</a></text>
<text top="442" left="699" width="4" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">;</a></text>
<text top="442" left="708" width="98" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Almeida et al.,</a></text>
<text top="460" left="459" width="30" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">2010</a></text>
<text top="460" left="489" width="317" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">). </a>Three different versions are implemented and</text>
<text top="478" left="459" width="346" height="15" font="0">the traditional backpropagation is included for com-</text>
<text top="496" left="459" width="346" height="15" font="0">parison purposes. Due to a custom-choice of acti-</text>
<text top="514" left="459" width="346" height="15" font="0">vation and error function, the package is very flex-</text>
<text top="531" left="459" width="346" height="15" font="0">ible. The user is able to use several hidden layers,</text>
<text top="549" left="459" width="346" height="15" font="0">which can reduce the computational costs by includ-</text>
<text top="567" left="459" width="346" height="15" font="0">ing an extra hidden layer and hence reducing the</text>
<text top="585" left="459" width="346" height="15" font="0">neurons per layer. We successfully used this package</text>
<text top="603" left="459" width="346" height="15" font="0">to model complex diseases, i.e. different structures</text>
<text top="621" left="459" width="249" height="15" font="0">of biological gene-gene interactions <a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">(</a></text>
<text top="621" left="708" width="98" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Günther et al.,</a></text>
<text top="639" left="459" width="30" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">2009</a></text>
<text top="639" left="489" width="317" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">). </a>Summarizing, <b>neuralnet </b>closes a gap concern-</text>
<text top="657" left="459" width="346" height="15" font="0">ing the provided algorithms for training neural net-</text>
<text top="675" left="459" width="75" height="15" font="0">works in R.</text>
<text top="693" left="481" width="324" height="15" font="0">To facilitate the usage of this package for new</text>
<text top="711" left="459" width="346" height="15" font="0">users of artificial neural networks, a brief introduc-</text>
<text top="729" left="459" width="346" height="15" font="0">tion to neural networks and the learning algorithms</text>
<text top="747" left="459" width="346" height="15" font="0">implemented in <b>neuralnet </b>is given before describing</text>
<text top="765" left="459" width="97" height="15" font="0">its application.</text>
<text top="812" left="459" width="234" height="21" font="4"><b>Multi-layer perceptrons</b></text>
<text top="850" left="459" width="346" height="15" font="0">The package <b>neuralnet </b>focuses on multi-layer per-</text>
<text top="868" left="459" width="100" height="15" font="0">ceptrons (MLP,</text>
<text top="868" left="564" width="84" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Bishop, 1995</a></text>
<text top="868" left="648" width="157" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">), </a>which are well appli-</text>
<text top="886" left="459" width="346" height="15" font="0">cable when modeling functional relationships. The</text>
<text top="904" left="459" width="346" height="15" font="0">underlying structure of an MLP is a directed graph,</text>
<text top="922" left="459" width="346" height="15" font="0">i.e. it consists of vertices and directed edges, in this</text>
<text top="940" left="459" width="346" height="15" font="0">context called neurons and synapses. The neurons</text>
<text top="958" left="459" width="346" height="15" font="0">are organized in layers, which are usually fully con-</text>
<text top="976" left="459" width="346" height="15" font="0">nected by synapses. In <b>neuralnet</b>, a synapse can only</text>
<text top="994" left="459" width="346" height="15" font="0">connect to subsequent layers. The input layer con-</text>
<text top="1012" left="459" width="346" height="15" font="0">sists of all covariates in separate neurons and the out-</text>
<text top="1030" left="459" width="346" height="15" font="0">put layer consists of the response variables. The lay-</text>
<text top="1048" left="459" width="346" height="15" font="0">ers in between are referred to as hidden layers, as</text>
<text top="1065" left="459" width="346" height="15" font="0">they are not directly observable. Input layer and hid-</text>
<text top="1083" left="459" width="346" height="15" font="0">den layers include a constant neuron relating to in-</text>
<text top="1101" left="459" width="346" height="15" font="0">tercept synapses, i.e. synapses that are not directly</text>
<text top="1119" left="459" width="233" height="15" font="0">influenced by any covariate. Figure</text>
<text top="1119" left="696" width="7" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#2">1</a></text>
<text top="1119" left="707" width="98" height="15" font="0">gives an exam-</text>
<text top="1137" left="459" width="346" height="15" font="0">ple of a neural network with one hidden layer that</text>
<text top="1155" left="459" width="346" height="15" font="0">consists of three hidden neurons. This neural net-</text>
<text top="1173" left="459" width="346" height="15" font="0">work models the relationship between the two co-</text>
<text top="1218" left="82" width="220" height="15" font="0">The R Journal Vol. 2/1, June 2010</text>
<text top="1218" left="704" width="102" height="15" font="0">ISSN 2073-4859</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="1262" width="892">
	<fontspec id="6" size="13" family="Times" color="#000000"/>
	<fontspec id="7" size="18" family="Times" color="#000000"/>
	<fontspec id="8" size="9" family="Times" color="#000000"/>
	<fontspec id="9" size="7" family="Times" color="#000000"/>
	<fontspec id="10" size="6" family="Times" color="#000000"/>
	<fontspec id="11" size="15" family="Times" color="#000000"/>
<text top="67" left="83" width="11" height="15" font="0">C</text>
<text top="69" left="94" width="86" height="12" font="1">ONTRIBUTED</text>
<text top="67" left="184" width="10" height="15" font="0">R</text>
<text top="69" left="195" width="61" height="12" font="1">ESEARCH</text>
<text top="67" left="261" width="12" height="15" font="0">A</text>
<text top="69" left="273" width="53" height="12" font="1">RTICLES</text>
<text top="67" left="790" width="15" height="15" font="0">31</text>
<text top="125" left="82" width="346" height="15" font="0">variates A and B and the response variable Y. <b>neural-</b></text>
<text top="143" left="82" width="22" height="15" font="3"><b>net</b></text>
<text top="143" left="109" width="320" height="15" font="0">theoretically allows inclusion of arbitrary num-</text>
<text top="161" left="82" width="346" height="15" font="0">bers of covariates and response variables. However,</text>
<text top="179" left="82" width="346" height="15" font="0">there can occur convergence difficulties using a huge</text>
<text top="197" left="82" width="332" height="15" font="0">number of both covariates and response variables.</text>
<text top="436" left="82" width="346" height="15" font="0">Figure 1: Example of a neural network with two in-</text>
<text top="454" left="82" width="346" height="15" font="0">put neurons (A and B), one output neuron (Y) and</text>
<text top="472" left="82" width="346" height="15" font="0">one hidden layer consisting of three hidden neurons.</text>
<text top="507" left="105" width="324" height="15" font="0">To each of the synapses, a weight is attached in-</text>
<text top="525" left="82" width="346" height="15" font="0">dicating the effect of the corresponding neuron, and</text>
<text top="543" left="82" width="346" height="15" font="0">all data pass the neural network as signals. The sig-</text>
<text top="560" left="82" width="346" height="15" font="0">nals are processed first by the so-called integration</text>
<text top="578" left="82" width="346" height="15" font="0">function combining all incoming signals and second</text>
<text top="596" left="82" width="346" height="15" font="0">by the so-called activation function transforming the</text>
<text top="614" left="82" width="141" height="15" font="0">output of the neuron.</text>
<text top="632" left="105" width="324" height="15" font="0">The simplest multi-layer perceptron (also known</text>
<text top="650" left="82" width="346" height="15" font="0">as perceptron) consists of an input layer with n co-</text>
<text top="668" left="82" width="346" height="15" font="0">variates and an output layer with one output neuron.</text>
<text top="686" left="82" width="162" height="15" font="0">It calculates the function</text>
<text top="724" left="114" width="7" height="15" font="0">o</text>
<text top="724" left="121" width="6" height="14" font="6">(</text>
<text top="724" left="128" width="7" height="15" font="3"><b>x</b></text>
<text top="724" left="136" width="22" height="14" font="6">) =</text>
<text top="724" left="163" width="4" height="15" font="0">f</text>
<text top="708" left="171" width="12" height="7" font="0"> </text>
<text top="724" left="184" width="11" height="15" font="0">w</text>
<text top="729" left="194" width="6" height="11" font="1">0</text>
<text top="724" left="204" width="12" height="14" font="6">+</text>
<text top="710" left="225" width="6" height="11" font="1">n</text>
<text top="718" left="220" width="16" height="27" font="7">∑</text>
<text top="743" left="219" width="19" height="11" font="1">i=1</text>
<text top="724" left="240" width="11" height="15" font="0">w</text>
<text top="729" left="251" width="3" height="11" font="1">i</text>
<text top="724" left="255" width="7" height="15" font="0">x</text>
<text top="729" left="263" width="3" height="11" font="1">i</text>
<text top="708" left="267" width="12" height="7" font="0">!</text>
<text top="724" left="283" width="12" height="14" font="6">=</text>
<text top="724" left="300" width="4" height="15" font="0">f</text>
<text top="717" left="309" width="9" height="7" font="0"></text>
<text top="724" left="318" width="11" height="15" font="0">w</text>
<text top="729" left="329" width="6" height="11" font="1">0</text>
<text top="724" left="338" width="12" height="14" font="6">+</text>
<text top="724" left="354" width="12" height="15" font="3"><b>w</b></text>
<text top="720" left="367" width="7" height="11" font="1">T</text>
<text top="724" left="375" width="7" height="15" font="3"><b>x</b></text>
<text top="717" left="383" width="9" height="7" font="0"></text>
<text top="724" left="394" width="4" height="15" font="0">,</text>
<text top="766" left="82" width="56" height="15" font="0">where w</text>
<text top="771" left="139" width="6" height="11" font="1">0</text>
<text top="766" left="149" width="159" height="15" font="0">denotes the intercept, <b>w</b></text>
<text top="765" left="312" width="22" height="14" font="6">= (</text>
<text top="765" left="334" width="11" height="15" font="0">w</text>
<text top="771" left="345" width="6" height="11" font="1">1</text>
<text top="766" left="351" width="39" height="15" font="0">, . . . , w</text>
<text top="770" left="391" width="6" height="11" font="1">n</text>
<text top="765" left="398" width="6" height="14" font="6">)</text>
<text top="766" left="408" width="21" height="15" font="0">the</text>
<text top="784" left="82" width="346" height="15" font="0">vector consisting of all synaptic weights without the</text>
<text top="801" left="82" width="102" height="15" font="0">intercept, and <b>x</b></text>
<text top="801" left="188" width="22" height="14" font="6">= (</text>
<text top="801" left="210" width="7" height="15" font="0">x</text>
<text top="807" left="218" width="6" height="11" font="1">1</text>
<text top="801" left="224" width="36" height="15" font="0">, . . . , x</text>
<text top="806" left="261" width="6" height="11" font="1">n</text>
<text top="801" left="268" width="6" height="14" font="6">)</text>
<text top="801" left="278" width="151" height="15" font="0">the vector of all covari-</text>
<text top="819" left="82" width="346" height="15" font="0">ates. The function is mathematically equivalent to</text>
<text top="837" left="82" width="225" height="15" font="0">that of GLM with link function f</text>
<text top="834" left="310" width="15" height="12" font="1">−1</text>
<text top="837" left="325" width="104" height="15" font="0">. Therefore, all</text>
<text top="855" left="82" width="346" height="15" font="0">calculated weights are in this case equivalent to the</text>
<text top="873" left="82" width="229" height="15" font="0">regression parameters of the GLM.</text>
<text top="891" left="105" width="324" height="15" font="0">To increase the modeling flexibility, hidden lay-</text>
<text top="909" left="82" width="210" height="15" font="0">ers can be included. However,</text>
<text top="909" left="298" width="126" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Hornik et al. (1989</a></text>
<text top="909" left="424" width="5" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">)</a></text>
<text top="927" left="82" width="346" height="15" font="0">showed that one hidden layer is sufficient to model</text>
<text top="945" left="82" width="346" height="15" font="0">any piecewise continuous function. Such an MLP</text>
<text top="963" left="82" width="346" height="15" font="0">with a hidden layer consisting of J hidden neurons</text>
<text top="981" left="82" width="219" height="15" font="0">calculates the following function:</text>
<text top="1023" left="113" width="7" height="15" font="0">o</text>
<text top="1023" left="120" width="6" height="14" font="6">(</text>
<text top="1023" left="126" width="7" height="15" font="3"><b>x</b></text>
<text top="1023" left="134" width="22" height="14" font="6">) =</text>
<text top="1023" left="158" width="4" height="15" font="0">f</text>
<text top="1007" left="166" width="12" height="7" font="0"> </text>
<text top="1023" left="179" width="11" height="15" font="0">w</text>
<text top="1028" left="189" width="6" height="11" font="1">0</text>
<text top="1023" left="199" width="12" height="14" font="6">+</text>
<text top="1008" left="222" width="4" height="11" font="1">J</text>
<text top="1017" left="216" width="16" height="27" font="7">∑</text>
<text top="1042" left="215" width="18" height="11" font="1">j=1</text>
<text top="1023" left="235" width="11" height="15" font="0">w</text>
<text top="1028" left="246" width="3" height="11" font="1">j</text>
<text top="1022" left="254" width="4" height="15" font="6">·</text>
<text top="1023" left="263" width="4" height="15" font="0">f</text>
<text top="1007" left="271" width="12" height="7" font="0"> </text>
<text top="1023" left="284" width="11" height="15" font="0">w</text>
<text top="1028" left="294" width="9" height="11" font="1">0j</text>
<text top="1023" left="308" width="12" height="14" font="6">+</text>
<text top="1009" left="329" width="6" height="11" font="1">n</text>
<text top="1017" left="324" width="16" height="27" font="7">∑</text>
<text top="1042" left="323" width="19" height="11" font="1">i=1</text>
<text top="1023" left="343" width="11" height="15" font="0">w</text>
<text top="1028" left="354" width="7" height="11" font="1">ij</text>
<text top="1023" left="363" width="7" height="15" font="0">x</text>
<text top="1028" left="371" width="3" height="11" font="1">i</text>
<text top="1007" left="375" width="24" height="7" font="0">!!</text>
<text top="1076" left="144" width="12" height="14" font="6">=</text>
<text top="1076" left="158" width="4" height="15" font="0">f</text>
<text top="1061" left="166" width="12" height="7" font="0"> </text>
<text top="1076" left="179" width="11" height="15" font="0">w</text>
<text top="1081" left="189" width="6" height="11" font="1">0</text>
<text top="1076" left="199" width="12" height="14" font="6">+</text>
<text top="1062" left="222" width="4" height="11" font="1">J</text>
<text top="1070" left="216" width="16" height="27" font="7">∑</text>
<text top="1095" left="215" width="18" height="11" font="1">j=1</text>
<text top="1076" left="235" width="11" height="15" font="0">w</text>
<text top="1082" left="246" width="3" height="11" font="1">j</text>
<text top="1075" left="254" width="4" height="15" font="6">·</text>
<text top="1076" left="263" width="4" height="15" font="0">f</text>
<text top="1070" left="271" width="9" height="7" font="0"></text>
<text top="1076" left="281" width="11" height="15" font="0">w</text>
<text top="1082" left="292" width="9" height="11" font="1">0j</text>
<text top="1076" left="305" width="12" height="14" font="6">+</text>
<text top="1076" left="320" width="12" height="15" font="3"><b>w</b></text>
<text top="1082" left="333" width="4" height="11" font="8"><b>j</b></text>
<text top="1073" left="338" width="7" height="11" font="1">T</text>
<text top="1076" left="346" width="7" height="15" font="3"><b>x</b></text>
<text top="1070" left="354" width="9" height="7" font="0"></text>
<text top="1061" left="363" width="12" height="7" font="0">!</text>
<text top="1076" left="377" width="4" height="15" font="0">,</text>
<text top="1119" left="82" width="56" height="15" font="0">where w</text>
<text top="1124" left="139" width="6" height="11" font="1">0</text>
<text top="1119" left="149" width="280" height="15" font="0">denotes the intercept of the output neuron</text>
<text top="1137" left="82" width="40" height="15" font="0">and w</text>
<text top="1143" left="122" width="9" height="11" font="1">0j</text>
<text top="1137" left="136" width="293" height="15" font="0">the intercept of the jth hidden neuron. Addi-</text>
<text top="1155" left="82" width="68" height="15" font="0">tionally, w</text>
<text top="1161" left="151" width="3" height="11" font="1">j</text>
<text top="1155" left="159" width="270" height="15" font="0">denotes the synaptic weight correspond-</text>
<text top="1173" left="82" width="346" height="15" font="0">ing to the synapse starting at the jth hidden neuron</text>
<text top="125" left="459" width="238" height="15" font="0">and leading to the output neuron, <b>w</b></text>
<text top="130" left="697" width="4" height="11" font="8"><b>j</b></text>
<text top="125" left="705" width="22" height="14" font="6">= (</text>
<text top="125" left="727" width="11" height="15" font="0">w</text>
<text top="131" left="738" width="9" height="11" font="1">1j</text>
<text top="125" left="749" width="39" height="15" font="0">, . . . , w</text>
<text top="130" left="788" width="10" height="11" font="1">nj</text>
<text top="125" left="799" width="6" height="14" font="6">)</text>
<text top="143" left="459" width="346" height="15" font="0">the vector of all synaptic weights corresponding to</text>
<text top="161" left="459" width="346" height="15" font="0">the synapses leading to the jth hidden neuron, and</text>
<text top="179" left="459" width="7" height="15" font="3"><b>x</b></text>
<text top="179" left="472" width="24" height="14" font="6">= (</text>
<text top="179" left="497" width="7" height="15" font="0">x</text>
<text top="184" left="505" width="6" height="11" font="1">1</text>
<text top="179" left="511" width="36" height="15" font="0">, . . . , x</text>
<text top="184" left="547" width="6" height="11" font="1">n</text>
<text top="179" left="555" width="6" height="14" font="6">)</text>
<text top="179" left="569" width="192" height="15" font="0">the vector of all covariates.</text>
<text top="179" left="777" width="29" height="15" font="0">This</text>
<text top="197" left="459" width="346" height="15" font="0">shows that neural networks are direct extensions of</text>
<text top="215" left="459" width="346" height="15" font="0">GLMs. However, the parameters, i.e. the weights,</text>
<text top="233" left="459" width="319" height="15" font="0">cannot be interpreted in the same way anymore.</text>
<text top="251" left="481" width="324" height="15" font="0">Formally stated, all hidden neurons and out-</text>
<text top="268" left="459" width="221" height="15" font="0">put neurons calculate an output f</text>
<text top="268" left="682" width="6" height="14" font="6">(</text>
<text top="268" left="689" width="7" height="15" font="0">g</text>
<text top="268" left="697" width="6" height="14" font="6">(</text>
<text top="268" left="703" width="7" height="15" font="0">z</text>
<text top="274" left="710" width="6" height="11" font="1">0</text>
<text top="268" left="717" width="12" height="15" font="0">, z</text>
<text top="274" left="729" width="6" height="11" font="1">1</text>
<text top="268" left="735" width="35" height="15" font="0">, . . . , z</text>
<text top="274" left="771" width="5" height="11" font="1">k</text>
<text top="268" left="777" width="28" height="14" font="6">)) =</text>
<text top="286" left="461" width="4" height="15" font="0">f</text>
<text top="286" left="468" width="6" height="14" font="6">(</text>
<text top="286" left="474" width="7" height="15" font="0">g</text>
<text top="286" left="482" width="6" height="14" font="6">(</text>
<text top="287" left="489" width="7" height="15" font="3"><b>z</b></text>
<text top="286" left="496" width="12" height="14" font="6">))</text>
<text top="286" left="515" width="290" height="15" font="0">from the outputs of all preceding neurons</text>
<text top="304" left="459" width="7" height="15" font="0">z</text>
<text top="310" left="466" width="6" height="11" font="1">0</text>
<text top="304" left="472" width="12" height="15" font="0">, z</text>
<text top="310" left="485" width="6" height="11" font="1">1</text>
<text top="304" left="491" width="35" height="15" font="0">, . . . , z</text>
<text top="310" left="526" width="5" height="11" font="1">k</text>
<text top="304" left="532" width="83" height="15" font="0">, where g : R</text>
<text top="301" left="615" width="20" height="11" font="1">k+1</text>
<text top="303" left="640" width="16" height="15" font="6">→</text>
<text top="305" left="659" width="11" height="13" font="0">R</text>
<text top="304" left="674" width="132" height="15" font="0">denotes the integra-</text>
<text top="322" left="459" width="152" height="15" font="0">tion function and f : R</text>
<text top="321" left="615" width="16" height="15" font="6">→</text>
<text top="323" left="635" width="11" height="13" font="0">R</text>
<text top="322" left="651" width="155" height="15" font="0">the activation function.</text>
<text top="340" left="459" width="90" height="15" font="0">The neuron z</text>
<text top="345" left="549" width="6" height="11" font="1">0</text>
<text top="339" left="559" width="12" height="15" font="6">≡</text>
<text top="340" left="576" width="229" height="15" font="0">1 is the constant one belonging to</text>
<text top="358" left="459" width="346" height="15" font="0">the intercept. The integration function is often de-</text>
<text top="378" left="459" width="62" height="15" font="0">fined as g</text>
<text top="378" left="521" width="6" height="14" font="6">(</text>
<text top="378" left="528" width="7" height="15" font="3"><b>z</b></text>
<text top="378" left="536" width="21" height="14" font="6">) =</text>
<text top="378" left="560" width="11" height="15" font="0">w</text>
<text top="383" left="571" width="6" height="11" font="1">0</text>
<text top="378" left="578" width="7" height="15" font="0">z</text>
<text top="383" left="585" width="6" height="11" font="1">0</text>
<text top="378" left="594" width="12" height="14" font="6">+</text>
<text top="376" left="608" width="11" height="19" font="0">∑</text>
<text top="375" left="620" width="5" height="11" font="1">k</text>
<text top="385" left="620" width="19" height="11" font="1">i=1</text>
<text top="378" left="641" width="11" height="15" font="0">w</text>
<text top="384" left="652" width="3" height="11" font="1">i</text>
<text top="378" left="657" width="7" height="15" font="0">z</text>
<text top="384" left="663" width="3" height="11" font="1">i</text>
<text top="378" left="671" width="12" height="14" font="6">=</text>
<text top="378" left="686" width="11" height="15" font="0">w</text>
<text top="383" left="697" width="6" height="11" font="1">0</text>
<text top="378" left="706" width="12" height="14" font="6">+</text>
<text top="378" left="721" width="12" height="15" font="3"><b>w</b></text>
<text top="375" left="734" width="8" height="11" font="8"><b>T</b></text>
<text top="378" left="742" width="7" height="15" font="3"><b>z</b></text>
<text top="378" left="750" width="55" height="15" font="0">. The ac-</text>
<text top="396" left="459" width="346" height="15" font="0">tivation function f is usually a bounded nondecreas-</text>
<text top="414" left="459" width="346" height="15" font="0">ing nonlinear and differentiable function such as the</text>
<text top="434" left="459" width="120" height="15" font="0">logistic function ( f</text>
<text top="433" left="581" width="6" height="14" font="6">(</text>
<text top="433" left="588" width="8" height="15" font="0">u</text>
<text top="433" left="596" width="21" height="14" font="6">) =</text>
<text top="430" left="636" width="6" height="11" font="1">1</text>
<text top="442" left="622" width="20" height="11" font="1">1+e</text>
<text top="440" left="642" width="7" height="9" font="9">−</text>
<text top="440" left="649" width="5" height="9" font="10">u</text>
<text top="434" left="657" width="148" height="15" font="0">) or the hyperbolic tan-</text>
<text top="452" left="459" width="346" height="15" font="0">gent. It should be chosen in relation to the response</text>
<text top="469" left="459" width="346" height="15" font="0">variable as it is the case in GLMs. The logistic func-</text>
<text top="487" left="459" width="346" height="15" font="0">tion is, for instance, appropriate for binary response</text>
<text top="505" left="459" width="346" height="15" font="0">variables since it maps the output of each neuron to</text>
<text top="523" left="459" width="76" height="15" font="0">the interval</text>
<text top="523" left="538" width="4" height="14" font="6">[</text>
<text top="523" left="543" width="20" height="15" font="0">0, 1</text>
<text top="523" left="563" width="4" height="14" font="6">]</text>
<text top="523" left="568" width="238" height="15" font="0">. At the moment, <b>neuralnet </b>uses the</text>
<text top="541" left="459" width="346" height="15" font="0">same integration as well as activation function for all</text>
<text top="559" left="459" width="57" height="15" font="0">neurons.</text>
<text top="600" left="459" width="166" height="17" font="11"><b>Supervised learning</b></text>
<text top="631" left="459" width="346" height="15" font="0">Neural networks are fitted to the data by learn-</text>
<text top="649" left="459" width="346" height="15" font="0">ing algorithms during a training process. <b>neuralnet</b></text>
<text top="667" left="459" width="346" height="15" font="0">focuses on supervised learning algorithms. These</text>
<text top="685" left="459" width="346" height="15" font="0">learning algorithms are characterized by the usage</text>
<text top="703" left="459" width="346" height="15" font="0">of a given output that is compared to the predicted</text>
<text top="721" left="459" width="346" height="15" font="0">output and by the adaptation of all parameters ac-</text>
<text top="738" left="459" width="346" height="15" font="0">cording to this comparison. The parameters of a neu-</text>
<text top="756" left="459" width="346" height="15" font="0">ral network are its weights. All weights are usually</text>
<text top="774" left="459" width="346" height="15" font="0">initialized with random values drawn from a stan-</text>
<text top="792" left="459" width="346" height="15" font="0">dard normal distribution. During an iterative train-</text>
<text top="810" left="459" width="297" height="15" font="0">ing process, the following steps are repeated:</text>
<text top="835" left="481" width="303" height="19" font="0"> The neural network calculates an output <b>o</b></text>
<text top="838" left="785" width="6" height="14" font="6">(</text>
<text top="839" left="791" width="7" height="15" font="3"><b>x</b></text>
<text top="838" left="799" width="6" height="14" font="6">)</text>
<text top="856" left="496" width="309" height="15" font="0">for given inputs <b>x </b>and current weights. If the</text>
<text top="874" left="496" width="309" height="15" font="0">training process is not yet completed, the pre-</text>
<text top="892" left="496" width="309" height="15" font="0">dicted output <b>o </b>will differ from the observed</text>
<text top="910" left="496" width="61" height="15" font="0">output <b>y</b>.</text>
<text top="936" left="481" width="324" height="19" font="0"> An error function E, like the sum of squared er-</text>
<text top="957" left="496" width="65" height="15" font="0">rors (SSE)</text>
<text top="1000" left="572" width="9" height="15" font="0">E</text>
<text top="1000" left="585" width="12" height="14" font="6">=</text>
<text top="990" left="602" width="7" height="15" font="0">1</text>
<text top="1011" left="602" width="7" height="15" font="0">2</text>
<text top="987" left="620" width="6" height="11" font="1">L</text>
<text top="994" left="615" width="16" height="27" font="7">∑</text>
<text top="1019" left="613" width="19" height="11" font="1">l=1</text>
<text top="987" left="640" width="9" height="11" font="1">H</text>
<text top="994" left="636" width="16" height="27" font="7">∑</text>
<text top="1019" left="634" width="21" height="11" font="1">h=1</text>
<text top="1000" left="657" width="6" height="14" font="6">(</text>
<text top="1000" left="663" width="7" height="15" font="0">o</text>
<text top="1006" left="670" width="10" height="11" font="1">lh</text>
<text top="999" left="684" width="12" height="15" font="6">−</text>
<text top="1000" left="699" width="7" height="15" font="0">y</text>
<text top="1006" left="707" width="10" height="11" font="1">lh</text>
<text top="1000" left="717" width="6" height="14" font="6">)</text>
<text top="997" left="724" width="6" height="11" font="1">2</text>
<text top="1044" left="496" width="132" height="15" font="0">or the cross-entropy</text>
<text top="1088" left="531" width="9" height="15" font="0">E</text>
<text top="1088" left="544" width="27" height="14" font="6">= −</text>
<text top="1074" left="581" width="6" height="11" font="1">L</text>
<text top="1082" left="576" width="16" height="27" font="7">∑</text>
<text top="1107" left="575" width="19" height="11" font="1">l=1</text>
<text top="1074" left="602" width="9" height="11" font="1">H</text>
<text top="1082" left="598" width="16" height="27" font="7">∑</text>
<text top="1107" left="596" width="21" height="11" font="1">h=1</text>
<text top="1088" left="618" width="6" height="14" font="6">(</text>
<text top="1088" left="625" width="7" height="15" font="0">y</text>
<text top="1093" left="633" width="10" height="11" font="1">lh</text>
<text top="1088" left="645" width="21" height="15" font="0">log</text>
<text top="1088" left="666" width="6" height="14" font="6">(</text>
<text top="1088" left="672" width="7" height="15" font="0">o</text>
<text top="1093" left="679" width="10" height="11" font="1">lh</text>
<text top="1088" left="690" width="6" height="14" font="6">)</text>
<text top="1124" left="608" width="21" height="14" font="6">+ (</text>
<text top="1124" left="630" width="7" height="15" font="0">1</text>
<text top="1123" left="640" width="12" height="15" font="6">−</text>
<text top="1124" left="655" width="7" height="15" font="0">y</text>
<text top="1130" left="663" width="10" height="11" font="1">lh</text>
<text top="1124" left="674" width="6" height="14" font="6">)</text>
<text top="1124" left="682" width="21" height="15" font="0">log</text>
<text top="1124" left="703" width="6" height="14" font="6">(</text>
<text top="1124" left="709" width="7" height="15" font="0">1</text>
<text top="1123" left="720" width="12" height="15" font="6">−</text>
<text top="1124" left="735" width="7" height="15" font="0">o</text>
<text top="1130" left="742" width="10" height="11" font="1">lh</text>
<text top="1124" left="752" width="12" height="14" font="6">))</text>
<text top="1124" left="767" width="4" height="15" font="0">,</text>
<text top="1155" left="496" width="309" height="15" font="0">measures the difference between predicted and</text>
<text top="1173" left="496" width="166" height="15" font="0">observed output, where l</text>
<text top="1173" left="666" width="12" height="14" font="6">=</text>
<text top="1173" left="682" width="123" height="15" font="0">1, . . . , L indexes the</text>
<text top="1218" left="82" width="220" height="15" font="0">The R Journal Vol. 2/1, June 2010</text>
<text top="1218" left="704" width="102" height="15" font="0">ISSN 2073-4859</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="1262" width="892">
	<fontspec id="12" size="12" family="Times" color="#000000"/>
	<fontspec id="13" size="11" family="Times" color="#000000"/>
<text top="67" left="82" width="15" height="15" font="0">32</text>
<text top="67" left="562" width="11" height="15" font="0">C</text>
<text top="69" left="573" width="86" height="12" font="1">ONTRIBUTED</text>
<text top="67" left="664" width="10" height="15" font="0">R</text>
<text top="69" left="674" width="61" height="12" font="1">ESEARCH</text>
<text top="67" left="740" width="12" height="15" font="0">A</text>
<text top="69" left="752" width="53" height="12" font="1">RTICLES</text>
<text top="125" left="120" width="309" height="15" font="0">observations, i.e. given input-output pairs, and</text>
<text top="143" left="120" width="7" height="15" font="0">h</text>
<text top="143" left="131" width="12" height="14" font="6">=</text>
<text top="143" left="147" width="169" height="15" font="0">1, . . . , H the output nodes.</text>
<text top="168" left="105" width="324" height="19" font="0"> All weights are adapted according to the rule</text>
<text top="190" left="120" width="156" height="15" font="0">of a learning algorithm.</text>
<text top="217" left="82" width="346" height="15" font="0">The process stops if a pre-specified criterion is ful-</text>
<text top="235" left="82" width="346" height="15" font="0">filled, e.g. if all absolute partial derivatives of the er-</text>
<text top="253" left="82" width="346" height="15" font="0">ror function with respect to the weights (<i>∂</i>E/<i>∂</i>w) are</text>
<text top="271" left="82" width="346" height="15" font="0">smaller than a given threshold. A widely used learn-</text>
<text top="289" left="82" width="346" height="15" font="0">ing algorithm is the resilient backpropagation algo-</text>
<text top="307" left="82" width="41" height="15" font="0">rithm.</text>
<text top="347" left="82" width="325" height="15" font="3"><b>Backpropagation and resilient backpropagation</b></text>
<text top="376" left="82" width="346" height="15" font="0">The resilient backpropagation algorithm is based on</text>
<text top="394" left="82" width="346" height="15" font="0">the traditional backpropagation algorithm that mod-</text>
<text top="411" left="82" width="346" height="15" font="0">ifies the weights of a neural network in order to find</text>
<text top="429" left="82" width="346" height="15" font="0">a local minimum of the error function. Therefore, the</text>
<text top="447" left="82" width="346" height="15" font="0">gradient of the error function (dE/d<b>w</b>) is calculated</text>
<text top="465" left="82" width="346" height="15" font="0">with respect to the weights in order to find a root. In</text>
<text top="483" left="82" width="346" height="15" font="0">particular, the weights are modified going in the op-</text>
<text top="501" left="82" width="346" height="15" font="0">posite direction of the partial derivatives until a local</text>
<text top="519" left="82" width="346" height="15" font="0">minimum is reached. This basic idea is roughly illus-</text>
<text top="537" left="82" width="103" height="15" font="0">trated in Figure</text>
<text top="537" left="189" width="7" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#3">2</a></text>
<text top="537" left="200" width="202" height="15" font="0">for a univariate error-function.</text>
<text top="762" left="82" width="60" height="15" font="0">Figure 2:</text>
<text top="762" left="158" width="271" height="15" font="0">Basic idea of the backpropagation algo-</text>
<text top="780" left="82" width="319" height="15" font="0">rithm illustrated for a univariate error function E</text>
<text top="780" left="401" width="6" height="14" font="6">(</text>
<text top="780" left="408" width="11" height="15" font="0">w</text>
<text top="780" left="419" width="6" height="14" font="6">)</text>
<text top="780" left="425" width="4" height="15" font="0">.</text>
<text top="814" left="105" width="324" height="15" font="0">If the partial derivative is negative, the weight is</text>
<text top="832" left="82" width="346" height="15" font="0">increased (left part of the figure); if the partial deriva-</text>
<text top="850" left="82" width="346" height="15" font="0">tive is positive, the weight is decreased (right part</text>
<text top="868" left="82" width="346" height="15" font="0">of the figure). This ensures that a local minimum is</text>
<text top="886" left="82" width="346" height="15" font="0">reached. All partial derivatives are calculated using</text>
<text top="904" left="82" width="346" height="15" font="0">the chain rule since the calculated function of a neu-</text>
<text top="922" left="82" width="346" height="15" font="0">ral network is basically a composition of integration</text>
<text top="940" left="82" width="346" height="15" font="0">and activation functions. A detailed explanation is</text>
<text top="958" left="82" width="54" height="15" font="0">given in</text>
<text top="958" left="140" width="74" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Rojas (1996</a></text>
<text top="958" left="214" width="9" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">).</a></text>
<text top="976" left="105" width="66" height="15" font="3"><b>neuralnet</b></text>
<text top="976" left="174" width="255" height="15" font="0">provides the opportunity to switch be-</text>
<text top="994" left="82" width="346" height="15" font="0">tween backpropagation, resilient backpropagation</text>
<text top="1012" left="82" width="39" height="15" font="0">with <a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">(</a></text>
<text top="1012" left="122" width="106" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Riedmiller, 1994</a></text>
<text top="1012" left="228" width="201" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">) </a>or without weight backtrack-</text>
<text top="1030" left="82" width="32" height="15" font="0">ing <a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">(</a></text>
<text top="1030" left="115" width="186" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Riedmiller and Braun, 1993</a></text>
<text top="1030" left="301" width="128" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">) </a>and the modified</text>
<text top="1048" left="82" width="214" height="15" font="0">globally convergent version by</text>
<text top="1048" left="303" width="126" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Anastasiadis et al.</a></text>
<text top="1065" left="82" width="35" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">(2005</a></text>
<text top="1065" left="117" width="312" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">). </a>All algorithms try to minimize the error func-</text>
<text top="1083" left="82" width="346" height="15" font="0">tion by adding a learning rate to the weights going</text>
<text top="1101" left="82" width="346" height="15" font="0">into the opposite direction of the gradient. Unlike</text>
<text top="1119" left="82" width="346" height="15" font="0">the traditional backpropagation algorithm, a sepa-</text>
<text top="1137" left="82" width="129" height="15" font="0">rate learning rate <i>η</i></text>
<text top="1143" left="212" width="5" height="11" font="1">k</text>
<text top="1137" left="218" width="211" height="15" font="0">, which can be changed during</text>
<text top="1155" left="82" width="346" height="15" font="0">the training process, is used for each weight in re-</text>
<text top="1173" left="82" width="346" height="15" font="0">silient backpropagation. This solves the problem of</text>
<text top="125" left="459" width="346" height="15" font="0">defining an over-all learning rate that is appropri-</text>
<text top="143" left="459" width="346" height="15" font="0">ate for the whole training process and the entire net-</text>
<text top="161" left="459" width="346" height="15" font="0">work. Additionally, instead of the magnitude of the</text>
<text top="179" left="459" width="346" height="15" font="0">partial derivatives only their sign is used to update</text>
<text top="197" left="459" width="346" height="15" font="0">the weights. This guarantees an equal influence of</text>
<text top="215" left="459" width="277" height="15" font="0">the learning rate over the entire network <a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">(</a></text>
<text top="215" left="735" width="70" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Riedmiller</a></text>
<text top="233" left="459" width="110" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">and Braun, 1993</a></text>
<text top="233" left="569" width="237" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">). </a>The weights are adjusted by the</text>
<text top="251" left="459" width="94" height="15" font="0">following rule</text>
<text top="298" left="515" width="11" height="15" font="0">w</text>
<text top="292" left="527" width="29" height="11" font="1">(t+1)</text>
<text top="306" left="526" width="5" height="11" font="1">k</text>
<text top="297" left="560" width="12" height="14" font="6">=</text>
<text top="298" left="575" width="11" height="15" font="0">w</text>
<text top="292" left="587" width="14" height="11" font="1">(t)</text>
<text top="306" left="586" width="5" height="11" font="1">k</text>
<text top="297" left="604" width="12" height="15" font="6">−</text>
<text top="301" left="619" width="8" height="11" font="12"><i>η</i></text>
<text top="292" left="628" width="14" height="11" font="1">(t)</text>
<text top="306" left="627" width="5" height="11" font="1">k</text>
<text top="297" left="645" width="4" height="15" font="6">·</text>
<text top="298" left="653" width="28" height="15" font="0">sign</text>
<text top="282" left="682" width="12" height="7" font="0"> </text>
<text top="291" left="697" width="17" height="11" font="12"><i>∂</i>E</text>
<text top="285" left="714" width="14" height="11" font="1">(t)</text>
<text top="316" left="696" width="18" height="11" font="12"><i>∂</i>w</text>
<text top="307" left="715" width="14" height="11" font="1">(t)</text>
<text top="321" left="715" width="5" height="11" font="1">k</text>
<text top="282" left="732" width="12" height="7" font="0">!</text>
<text top="298" left="745" width="4" height="15" font="0">,</text>
<text top="346" left="459" width="91" height="15" font="0">as opposed to</text>
<text top="391" left="550" width="11" height="15" font="0">w</text>
<text top="386" left="561" width="29" height="11" font="1">(t+1)</text>
<text top="399" left="561" width="5" height="11" font="1">k</text>
<text top="391" left="595" width="12" height="14" font="6">=</text>
<text top="391" left="610" width="11" height="15" font="0">w</text>
<text top="386" left="621" width="14" height="11" font="1">(t)</text>
<text top="399" left="621" width="5" height="11" font="1">k</text>
<text top="390" left="639" width="12" height="15" font="6">−</text>
<text top="395" left="654" width="8" height="11" font="12"><i>η</i></text>
<text top="390" left="666" width="4" height="15" font="6">·</text>
<text top="385" left="676" width="17" height="11" font="12"><i>∂</i>E</text>
<text top="378" left="693" width="14" height="11" font="1">(t)</text>
<text top="410" left="675" width="18" height="11" font="12"><i>∂</i>w</text>
<text top="401" left="694" width="14" height="11" font="1">(t)</text>
<text top="415" left="694" width="5" height="11" font="1">k</text>
<text top="391" left="710" width="4" height="15" font="0">,</text>
<text top="440" left="459" width="346" height="15" font="0">in traditional backpropagation, where t indexes the</text>
<text top="458" left="459" width="217" height="15" font="0">iteration steps and k the weights.</text>
<text top="476" left="481" width="324" height="15" font="0">In order to speed up convergence in shallow ar-</text>
<text top="494" left="459" width="153" height="15" font="0">eas, the learning rate <i>η</i></text>
<text top="499" left="612" width="5" height="11" font="1">k</text>
<text top="494" left="623" width="183" height="15" font="0">will be increased if the cor-</text>
<text top="512" left="459" width="346" height="15" font="0">responding partial derivative keeps its sign. On the</text>
<text top="530" left="459" width="346" height="15" font="0">contrary, it will be decreased if the partial derivative</text>
<text top="548" left="459" width="346" height="15" font="0">of the error function changes its sign since a chang-</text>
<text top="566" left="459" width="346" height="15" font="0">ing sign indicates that the minimum is missed due</text>
<text top="583" left="459" width="346" height="15" font="0">to a too large learning rate. Weight backtracking is</text>
<text top="601" left="459" width="346" height="15" font="0">a technique of undoing the last iteration and adding</text>
<text top="619" left="459" width="346" height="15" font="0">a smaller value to the weight in the next step. With-</text>
<text top="637" left="459" width="346" height="15" font="0">out the usage of weight backtracking, the algorithm</text>
<text top="655" left="459" width="346" height="15" font="0">can jump over the minimum several times. For ex-</text>
<text top="673" left="459" width="346" height="15" font="0">ample, the pseudocode of resilient backpropagation</text>
<text top="691" left="459" width="248" height="15" font="0">with weight backtracking is given by <a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">(</a></text>
<text top="691" left="707" width="98" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Riedmiller and</a></text>
<text top="709" left="459" width="78" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Braun, 1993</a></text>
<text top="709" left="536" width="5" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">)</a></text>
<text top="743" left="459" width="110" height="10" font="13">for all weights{</text>
<text top="760" left="486" width="144" height="10" font="13">if (grad.old*grad&gt;0){</text>
<text top="776" left="514" width="268" height="10" font="13">delta := min(delta*eta.plus, delta.max)</text>
<text top="793" left="514" width="254" height="10" font="13">weights := weights - sign(grad)*delta</text>
<text top="809" left="514" width="110" height="10" font="13">grad.old := grad</text>
<text top="826" left="486" width="7" height="10" font="13">}</text>
<text top="842" left="486" width="178" height="10" font="13">else if (grad.old*grad&lt;0){</text>
<text top="858" left="514" width="281" height="10" font="13">weights := weights + sign(grad.old)*delta</text>
<text top="875" left="514" width="274" height="10" font="13">delta := max(delta*eta.minus, delta.min)</text>
<text top="891" left="514" width="89" height="10" font="13">grad.old := 0</text>
<text top="908" left="486" width="7" height="10" font="13">}</text>
<text top="924" left="486" width="178" height="10" font="13">else if (grad.old*grad=0){</text>
<text top="941" left="514" width="254" height="10" font="13">weights := weights - sign(grad)*delta</text>
<text top="957" left="514" width="110" height="10" font="13">grad.old := grad</text>
<text top="973" left="486" width="7" height="10" font="13">}</text>
<text top="990" left="459" width="7" height="10" font="13">}</text>
<text top="1021" left="459" width="346" height="15" font="0">while that of the regular backpropagation is given by</text>
<text top="1055" left="459" width="110" height="10" font="13">for all weights{</text>
<text top="1072" left="486" width="213" height="10" font="13">weights := weights - grad*delta</text>
<text top="1088" left="459" width="7" height="10" font="13">}</text>
<text top="1119" left="481" width="324" height="15" font="0">The globally convergent version introduced by</text>
<text top="1137" left="459" width="163" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Anastasiadis et al. (2005</a></text>
<text top="1137" left="622" width="183" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">) </a>performs a resilient back-</text>
<text top="1155" left="459" width="346" height="15" font="0">propagation with an additional modification of one</text>
<text top="1173" left="459" width="346" height="15" font="0">learning rate in relation to all other learning rates. It</text>
<text top="1218" left="82" width="220" height="15" font="0">The R Journal Vol. 2/1, June 2010</text>
<text top="1218" left="704" width="102" height="15" font="0">ISSN 2073-4859</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="1262" width="892">
	<fontspec id="14" size="9" family="Times" color="#000000"/>
<text top="67" left="83" width="11" height="15" font="0">C</text>
<text top="69" left="94" width="86" height="12" font="1">ONTRIBUTED</text>
<text top="67" left="184" width="10" height="15" font="0">R</text>
<text top="69" left="195" width="61" height="12" font="1">ESEARCH</text>
<text top="67" left="261" width="12" height="15" font="0">A</text>
<text top="69" left="273" width="53" height="12" font="1">RTICLES</text>
<text top="67" left="790" width="15" height="15" font="0">33</text>
<text top="125" left="82" width="346" height="15" font="0">is either the learning rate associated with the small-</text>
<text top="143" left="82" width="346" height="15" font="0">est absolute partial derivative or the smallest learn-</text>
<text top="161" left="82" width="346" height="15" font="0">ing rate (indexed with i), that is changed according</text>
<text top="179" left="82" width="13" height="15" font="0">to</text>
<text top="222" left="160" width="8" height="11" font="12"><i>η</i></text>
<text top="213" left="169" width="14" height="11" font="1">(t)</text>
<text top="226" left="168" width="3" height="11" font="1">i</text>
<text top="218" left="186" width="28" height="14" font="6">= −</text>
<text top="195" left="217" width="11" height="19" font="0">∑</text>
<text top="204" left="228" width="26" height="11" font="1">k;k6=i</text>
<text top="201" left="258" width="8" height="11" font="12"><i>η</i></text>
<text top="192" left="266" width="14" height="11" font="1">(t)</text>
<text top="206" left="266" width="5" height="11" font="1">k</text>
<text top="197" left="284" width="4" height="15" font="6">·</text>
<text top="197" left="294" width="13" height="9" font="14"><i>∂</i>E</text>
<text top="192" left="307" width="4" height="8" font="9">(</text>
<text top="192" left="311" width="3" height="9" font="10">t</text>
<text top="192" left="314" width="4" height="8" font="9">)</text>
<text top="213" left="293" width="14" height="9" font="14"><i>∂</i>w</text>
<text top="206" left="307" width="4" height="8" font="9">(</text>
<text top="206" left="311" width="3" height="9" font="10">t</text>
<text top="206" left="314" width="4" height="8" font="9">)</text>
<text top="217" left="307" width="4" height="9" font="10">k</text>
<text top="198" left="324" width="12" height="14" font="6">+</text>
<text top="201" left="339" width="7" height="11" font="12"><i>δ</i></text>
<text top="233" left="269" width="13" height="9" font="14"><i>∂</i>E</text>
<text top="228" left="282" width="4" height="8" font="9">(</text>
<text top="228" left="286" width="3" height="9" font="10">t</text>
<text top="228" left="289" width="4" height="8" font="9">)</text>
<text top="249" left="268" width="14" height="9" font="14"><i>∂</i>w</text>
<text top="242" left="283" width="4" height="8" font="9">(</text>
<text top="242" left="287" width="3" height="9" font="10">t</text>
<text top="242" left="290" width="4" height="8" font="9">)</text>
<text top="252" left="283" width="2" height="9" font="10">i</text>
<text top="218" left="348" width="4" height="15" font="0">,</text>
<text top="274" left="82" width="9" height="15" font="0">if</text>
<text top="274" left="100" width="13" height="9" font="14"><i>∂</i>E</text>
<text top="269" left="113" width="4" height="8" font="9">(</text>
<text top="269" left="117" width="3" height="9" font="10">t</text>
<text top="269" left="120" width="4" height="8" font="9">)</text>
<text top="290" left="99" width="14" height="9" font="14"><i>∂</i>w</text>
<text top="283" left="114" width="4" height="8" font="9">(</text>
<text top="283" left="117" width="3" height="9" font="10">t</text>
<text top="283" left="121" width="4" height="8" font="9">)</text>
<text top="293" left="113" width="2" height="9" font="10">i</text>
<text top="273" left="131" width="12" height="15" font="6">6=</text>
<text top="274" left="148" width="51" height="15" font="0">0 and 0</text>
<text top="274" left="204" width="12" height="14" font="6">&lt;</text>
<text top="278" left="221" width="7" height="11" font="12"><i>δ</i></text>
<text top="273" left="232" width="16" height="15" font="6"></text>
<text top="271" left="253" width="176" height="19" font="0">∞. For further details see</text>
<text top="302" left="82" width="158" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Anastasiadis et al. (2005</a></text>
<text top="302" left="241" width="9" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">).</a></text>
<text top="348" left="82" width="158" height="21" font="4"><b>Using neuralnet</b></text>
<text top="387" left="82" width="66" height="15" font="3"><b>neuralnet</b></text>
<text top="387" left="152" width="277" height="15" font="0">depends on two other packages: <b>grid </b>and</text>
<text top="405" left="82" width="45" height="15" font="3"><b>MASS</b></text>
<text top="405" left="135" width="5" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">(</a></text>
<text top="405" left="140" width="182" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">Venables and Ripley, 2002</a></text>
<text top="405" left="323" width="9" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">).</a></text>
<text top="405" left="348" width="81" height="15" font="0">Its usage is</text>
<text top="423" left="82" width="346" height="15" font="0">leaned towards that of functions dealing with regres-</text>
<text top="441" left="82" width="346" height="15" font="0">sion analyses like lm and glm. As essential argu-</text>
<text top="458" left="82" width="346" height="15" font="0">ments, a formula in terms of response variables ˜ sum of</text>
<text top="476" left="82" width="346" height="15" font="0">covariates and a data set containing covariates and re-</text>
<text top="494" left="82" width="346" height="15" font="0">sponse variables have to be specified. Default values</text>
<text top="512" left="82" width="346" height="15" font="0">are defined for all other parameters (see next subsec-</text>
<text top="530" left="82" width="346" height="15" font="0">tion). We use the data set infert that is provided by</text>
<text top="548" left="82" width="346" height="15" font="0">the package <b>datasets </b>to illustrate its application. This</text>
<text top="566" left="82" width="346" height="15" font="0">data set contains data of a case-control study that in-</text>
<text top="584" left="82" width="346" height="15" font="0">vestigated infertility after spontaneous and induced</text>
<text top="602" left="82" width="64" height="15" font="0">abortion <a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">(</a></text>
<text top="602" left="147" width="158" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">Trichopoulos et al., 1976</a></text>
<text top="602" left="305" width="124" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">). </a>The data set con-</text>
<text top="620" left="82" width="346" height="15" font="0">sists of 248 observations, 83 women, who were infer-</text>
<text top="638" left="82" width="346" height="15" font="0">tile (cases), and 165 women, who were not infertile</text>
<text top="656" left="82" width="346" height="15" font="0">(controls). It includes amongst others the variables</text>
<text top="675" left="82" width="23" height="12" font="0">age</text>
<text top="674" left="105" width="324" height="15" font="0">, parity, induced, and spontaneous. The vari-</text>
<text top="692" left="82" width="346" height="15" font="0">ables induced and spontaneous denote the number</text>
<text top="710" left="82" width="346" height="15" font="0">of prior induced and spontaneous abortions, respec-</text>
<text top="727" left="82" width="346" height="15" font="0">tively. Both variables take possible values 0, 1, and</text>
<text top="745" left="82" width="346" height="15" font="0">2 relating to 0, 1, and 2 or more prior abortions. The</text>
<text top="763" left="82" width="346" height="15" font="0">age in years is given by the variable age and the num-</text>
<text top="781" left="82" width="154" height="15" font="0">ber of births by parity.</text>
<text top="823" left="82" width="230" height="17" font="11"><b>Training of neural networks</b></text>
<text top="853" left="82" width="346" height="15" font="0">The function neuralnet used for training a neural</text>
<text top="871" left="82" width="346" height="15" font="0">network provides the opportunity to define the re-</text>
<text top="889" left="82" width="346" height="15" font="0">quired number of hidden layers and hidden neurons</text>
<text top="907" left="82" width="346" height="15" font="0">according to the needed complexity. The complex-</text>
<text top="925" left="82" width="346" height="15" font="0">ity of the calculated function increases with the addi-</text>
<text top="943" left="82" width="346" height="15" font="0">tion of hidden layers or hidden neurons. The default</text>
<text top="961" left="82" width="346" height="15" font="0">value is one hidden layer with one hidden neuron.</text>
<text top="979" left="82" width="346" height="15" font="0">The most important arguments of the function are</text>
<text top="997" left="82" width="92" height="15" font="0">the following:</text>
<text top="1022" left="105" width="324" height="19" font="0"> formula, a symbolic description of the model to</text>
<text top="1043" left="120" width="215" height="15" font="0">be fitted (see above). No default.</text>
<text top="1069" left="105" width="324" height="19" font="0"> data, a data frame containing the variables</text>
<text top="1090" left="120" width="216" height="15" font="0">specified in formula. No default.</text>
<text top="1116" left="105" width="324" height="19" font="0"> hidden, a vector specifying the number of hid-</text>
<text top="1137" left="120" width="309" height="15" font="0">den layers and hidden neurons in each layer.</text>
<text top="1155" left="120" width="309" height="15" font="0">For example the vector (3,2,1) induces a neu-</text>
<text top="1173" left="120" width="309" height="15" font="0">ral network with three hidden layers, the first</text>
<text top="125" left="496" width="309" height="15" font="0">one with three, the second one with two and</text>
<text top="143" left="496" width="309" height="15" font="0">the third one with one hidden neuron. Default:</text>
<text top="161" left="496" width="11" height="15" font="0">1.</text>
<text top="185" left="481" width="324" height="19" font="0"> threshold, an integer specifying the threshold</text>
<text top="207" left="496" width="309" height="15" font="0">for the partial derivatives of the error function</text>
<text top="224" left="496" width="223" height="15" font="0">as stopping criteria. Default: 0.01.</text>
<text top="249" left="481" width="324" height="19" font="0"> rep, number of repetitions for the training pro-</text>
<text top="270" left="496" width="104" height="15" font="0">cess. Default: 1.</text>
<text top="294" left="481" width="324" height="19" font="0"> startweights, a vector containing prespecified</text>
<text top="316" left="496" width="309" height="15" font="0">starting values for the weights. Default: ran-</text>
<text top="334" left="496" width="309" height="15" font="0">dom numbers drawn from the standard normal</text>
<text top="352" left="496" width="78" height="15" font="0">distribution</text>
<text top="376" left="481" width="324" height="19" font="0"> algorithm, a string containing the algo-</text>
<text top="397" left="496" width="309" height="15" font="0">rithm type. Possible values are &#34;backprop&#34;,</text>
<text top="416" left="496" width="61" height="12" font="0">&#34;rprop+&#34;</text>
<text top="415" left="557" width="4" height="15" font="0">,</text>
<text top="416" left="582" width="61" height="12" font="0">&#34;rprop-&#34;</text>
<text top="415" left="643" width="4" height="15" font="0">,</text>
<text top="416" left="668" width="38" height="12" font="0">&#34;sag&#34;</text>
<text top="415" left="706" width="4" height="15" font="0">,</text>
<text top="415" left="732" width="14" height="15" font="0">or</text>
<text top="416" left="763" width="38" height="12" font="0">&#34;slr&#34;</text>
<text top="415" left="802" width="4" height="15" font="0">.</text>
<text top="434" left="496" width="76" height="12" font="0">&#34;backprop&#34;</text>
<text top="433" left="578" width="227" height="15" font="0">refers to traditional backpropaga-</text>
<text top="451" left="496" width="309" height="15" font="0">tion, &#34;rprop+&#34; and &#34;rprop-&#34; refer to resilient</text>
<text top="469" left="496" width="309" height="15" font="0">backpropagation with and without weight</text>
<text top="487" left="496" width="309" height="15" font="0">backtracking and &#34;sag&#34; and &#34;slr&#34; refer to the</text>
<text top="505" left="496" width="309" height="15" font="0">modified globally convergent algorithm (gr-</text>
<text top="523" left="496" width="309" height="15" font="0">prop). &#34;sag&#34; and &#34;slr&#34; define the learning rate</text>
<text top="541" left="496" width="309" height="15" font="0">that is changed according to all others. &#34;sag&#34;</text>
<text top="559" left="496" width="309" height="15" font="0">refers to the smallest absolute derivative, &#34;slr&#34;</text>
<text top="577" left="496" width="309" height="15" font="0">to the smallest learning rate. Default: &#34;rprop+&#34;</text>
<text top="601" left="481" width="324" height="19" font="0"> err.fct, a differentiable error function. The</text>
<text top="622" left="496" width="309" height="15" font="0">strings &#34;sse&#34; and &#34;ce&#34; can be used, which refer</text>
<text top="640" left="496" width="309" height="15" font="0">to ’sum of squared errors’ and ’cross entropy’.</text>
<text top="658" left="496" width="96" height="15" font="0">Default: &#34;sse&#34;</text>
<text top="682" left="481" width="324" height="19" font="0"> act.fct, a differentiable activation function.</text>
<text top="704" left="496" width="309" height="15" font="0">The strings &#34;logistic&#34; and &#34;tanh&#34; are possible</text>
<text top="722" left="496" width="309" height="15" font="0">for the logistic function and tangent hyperboli-</text>
<text top="740" left="496" width="164" height="15" font="0">cus. Default: &#34;logistic&#34;</text>
<text top="764" left="481" width="176" height="19" font="0"> linear.output, logical.</text>
<text top="767" left="678" width="127" height="15" font="0">If act.fct should</text>
<text top="785" left="496" width="309" height="15" font="0">not be applied to the output neurons,</text>
<text top="804" left="496" width="99" height="12" font="0">linear.output</text>
<text top="803" left="599" width="188" height="15" font="0">has to be TRUE. Default: TRUE</text>
<text top="827" left="481" width="324" height="19" font="0"> likelihood, logical. If the error function is</text>
<text top="849" left="496" width="309" height="15" font="0">equal to the negative log-likelihood function,</text>
<text top="868" left="496" width="76" height="12" font="0">likelihood</text>
<text top="867" left="578" width="228" height="15" font="0">has to be TRUE. Akaike’s Informa-</text>
<text top="885" left="496" width="135" height="15" font="0">tion Criterion (AIC,</text>
<text top="885" left="637" width="87" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Akaike, 1973</a></text>
<text top="885" left="724" width="81" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">) </a>and Bayes</text>
<text top="903" left="496" width="179" height="15" font="0">Information Criterion (BIC,</text>
<text top="903" left="679" width="93" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Schwarz, 1978</a></text>
<text top="903" left="772" width="34" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">) </a>will</text>
<text top="920" left="496" width="230" height="15" font="0">then be calculated. Default: FALSE</text>
<text top="945" left="481" width="324" height="19" font="0"> exclude, a vector or matrix specifying weights</text>
<text top="966" left="496" width="309" height="15" font="0">that should be excluded from training. A ma-</text>
<text top="984" left="496" width="309" height="15" font="0">trix with n rows and three columns will exclude</text>
<text top="1002" left="496" width="309" height="15" font="0">n weights, where the first column indicates the</text>
<text top="1020" left="496" width="309" height="15" font="0">layer, the second column the input neuron of</text>
<text top="1038" left="496" width="309" height="15" font="0">the weight, and the third neuron the output</text>
<text top="1056" left="496" width="309" height="15" font="0">neuron of the weight. If given as vector, the</text>
<text top="1074" left="496" width="309" height="15" font="0">exact numbering has to be known. The num-</text>
<text top="1092" left="496" width="309" height="15" font="0">bering can be checked using the provided plot</text>
<text top="1110" left="496" width="288" height="15" font="0">or the saved starting weights. Default: NULL</text>
<text top="1134" left="481" width="324" height="19" font="0"> constant.weights, a vector specifying the val-</text>
<text top="1155" left="496" width="309" height="15" font="0">ues of weights that are excluded from training</text>
<text top="1173" left="496" width="226" height="15" font="0">and treated as fixed. Default: NULL</text>
<text top="1218" left="82" width="220" height="15" font="0">The R Journal Vol. 2/1, June 2010</text>
<text top="1218" left="704" width="102" height="15" font="0">ISSN 2073-4859</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="1262" width="892">
<text top="67" left="82" width="15" height="15" font="0">34</text>
<text top="67" left="562" width="11" height="15" font="0">C</text>
<text top="69" left="573" width="86" height="12" font="1">ONTRIBUTED</text>
<text top="67" left="664" width="10" height="15" font="0">R</text>
<text top="69" left="674" width="61" height="12" font="1">ESEARCH</text>
<text top="67" left="740" width="12" height="15" font="0">A</text>
<text top="69" left="752" width="53" height="12" font="1">RTICLES</text>
<text top="125" left="105" width="324" height="15" font="0">The usage of neuralnet is described by model-</text>
<text top="143" left="82" width="346" height="15" font="0">ing the relationship between the case-control status</text>
<text top="161" left="82" width="346" height="15" font="0">(case) as response variable and the four covariates</text>
<text top="180" left="82" width="23" height="12" font="0">age</text>
<text top="179" left="105" width="324" height="15" font="0">, parity, induced and spontaneous. Since the</text>
<text top="197" left="82" width="346" height="15" font="0">response variable is binary, the activation function</text>
<text top="215" left="82" width="346" height="15" font="0">could be chosen as logistic function (default) and the</text>
<text top="233" left="82" width="346" height="15" font="0">error function as cross-entropy (err.fct=&#34;ce&#34;). Ad-</text>
<text top="251" left="82" width="346" height="15" font="0">ditionally, the item linear.output should be stated</text>
<text top="268" left="82" width="346" height="15" font="0">as FALSE to ensure that the output is mapped by the</text>
<text top="286" left="82" width="222" height="15" font="0">activation function to the interval</text>
<text top="286" left="309" width="4" height="14" font="6">[</text>
<text top="286" left="313" width="20" height="15" font="0">0, 1</text>
<text top="286" left="334" width="4" height="14" font="6">]</text>
<text top="286" left="338" width="91" height="15" font="0">. The number</text>
<text top="304" left="82" width="346" height="15" font="0">of hidden neurons should be determined in relation</text>
<text top="322" left="82" width="346" height="15" font="0">to the needed complexity. A neural network with for</text>
<text top="340" left="82" width="346" height="15" font="0">example two hidden neurons is trained by the fol-</text>
<text top="358" left="82" width="125" height="15" font="0">lowing statements:</text>
<text top="392" left="82" width="137" height="10" font="13">&gt; library(neuralnet)</text>
<text top="408" left="82" width="206" height="10" font="13">Loading required package: grid</text>
<text top="425" left="82" width="206" height="10" font="13">Loading required package: MASS</text>
<text top="441" left="82" width="7" height="10" font="13">&gt;</text>
<text top="458" left="82" width="123" height="10" font="13">&gt; nn &lt;- neuralnet(</text>
<text top="474" left="82" width="7" height="10" font="13">+</text>
<text top="474" left="137" width="247" height="10" font="13">case~age+parity+induced+spontaneous,</text>
<text top="491" left="82" width="7" height="10" font="13">+</text>
<text top="491" left="137" width="247" height="10" font="13">data=infert, hidden=2, err.fct=&#34;ce&#34;,</text>
<text top="507" left="82" width="7" height="10" font="13">+</text>
<text top="507" left="137" width="137" height="10" font="13">linear.output=FALSE)</text>
<text top="523" left="82" width="27" height="10" font="13">&gt; nn</text>
<text top="540" left="82" width="34" height="10" font="13">Call:</text>
<text top="556" left="96" width="69" height="10" font="13">neuralnet(</text>
<text top="573" left="96" width="316" height="10" font="13">formula = case~age+parity+induced+spontaneous,</text>
<text top="589" left="96" width="288" height="10" font="13">data = infert, hidden = 2, err.fct = &#34;ce&#34;,</text>
<text top="606" left="96" width="151" height="10" font="13">linear.output = FALSE)</text>
<text top="638" left="82" width="192" height="10" font="13">1 repetition was calculated.</text>
<text top="671" left="137" width="199" height="10" font="13">Error Reached Threshold Steps</text>
<text top="688" left="82" width="89" height="10" font="13">1 125.2126851</text>
<text top="688" left="199" width="96" height="10" font="13">0.008779243419</text>
<text top="688" left="309" width="27" height="10" font="13">5254</text>
<text top="718" left="105" width="324" height="15" font="0">Basic information about the training process and</text>
<text top="736" left="82" width="346" height="15" font="0">the trained neural network is saved in nn. This in-</text>
<text top="754" left="82" width="346" height="15" font="0">cludes all information that has to be known to repro-</text>
<text top="772" left="82" width="346" height="15" font="0">duce the results as for instance the starting weights.</text>
<text top="790" left="82" width="233" height="15" font="0">Important values are the following:</text>
<text top="820" left="105" width="324" height="19" font="0"> net.result, a list containing the overall result,</text>
<text top="841" left="120" width="309" height="15" font="0">i.e. the output, of the neural network for each</text>
<text top="859" left="120" width="74" height="15" font="0">replication.</text>
<text top="885" left="105" width="324" height="19" font="0"> weights, a list containing the fitted weights of</text>
<text top="907" left="120" width="261" height="15" font="0">the neural network for each replication.</text>
<text top="933" left="105" width="324" height="19" font="0"> generalized.weights, a list containing the</text>
<text top="955" left="120" width="309" height="15" font="0">generalized weights of the neural network for</text>
<text top="973" left="120" width="108" height="15" font="0">each replication.</text>
<text top="999" left="105" width="324" height="19" font="0"> result.matrix, a matrix containing the error,</text>
<text top="1021" left="120" width="309" height="15" font="0">reached threshold, needed steps, AIC and BIC</text>
<text top="1038" left="120" width="309" height="15" font="0">(computed if likelihood=TRUE) and estimated</text>
<text top="1056" left="120" width="309" height="15" font="0">weights for each replication. Each column rep-</text>
<text top="1074" left="120" width="152" height="15" font="0">resents one replication.</text>
<text top="1101" left="105" width="324" height="19" font="0"> startweights, a list containing the starting</text>
<text top="1122" left="120" width="187" height="15" font="0">weights for each replication.</text>
<text top="1155" left="105" width="324" height="15" font="0">A summary of the main results is provided by</text>
<text top="1174" left="82" width="122" height="12" font="0">nn$result.matrix</text>
<text top="1173" left="204" width="4" height="15" font="0">:</text>
<text top="127" left="459" width="123" height="10" font="13">&gt; nn$result.matrix</text>
<text top="144" left="733" width="7" height="10" font="13">1</text>
<text top="160" left="459" width="34" height="10" font="13">error</text>
<text top="160" left="630" width="110" height="10" font="13">125.212685099732</text>
<text top="177" left="459" width="117" height="10" font="13">reached.threshold</text>
<text top="177" left="644" width="96" height="10" font="13">0.008779243419</text>
<text top="193" left="459" width="34" height="10" font="13">steps</text>
<text top="193" left="623" width="117" height="10" font="13">5254.000000000000</text>
<text top="210" left="459" width="144" height="10" font="13">Intercept.to.1layhid1</text>
<text top="210" left="644" width="96" height="10" font="13">5.593787533788</text>
<text top="226" left="459" width="103" height="10" font="13">age.to.1layhid1</text>
<text top="226" left="637" width="103" height="10" font="13">-0.117576380283</text>
<text top="242" left="459" width="123" height="10" font="13">parity.to.1layhid1</text>
<text top="242" left="644" width="96" height="10" font="13">1.765945780047</text>
<text top="259" left="459" width="130" height="10" font="13">induced.to.1layhid1</text>
<text top="259" left="637" width="103" height="10" font="13">-2.200113693672</text>
<text top="275" left="459" width="158" height="10" font="13">spontaneous.to.1layhid1</text>
<text top="275" left="637" width="103" height="10" font="13">-3.369491912508</text>
<text top="292" left="459" width="144" height="10" font="13">Intercept.to.1layhid2</text>
<text top="292" left="644" width="96" height="10" font="13">1.060701883258</text>
<text top="308" left="459" width="103" height="10" font="13">age.to.1layhid2</text>
<text top="308" left="644" width="96" height="10" font="13">2.925601414213</text>
<text top="325" left="459" width="123" height="10" font="13">parity.to.1layhid2</text>
<text top="325" left="644" width="96" height="10" font="13">0.259809664488</text>
<text top="341" left="459" width="130" height="10" font="13">induced.to.1layhid2</text>
<text top="341" left="637" width="103" height="10" font="13">-0.120043540527</text>
<text top="357" left="459" width="158" height="10" font="13">spontaneous.to.1layhid2</text>
<text top="357" left="637" width="103" height="10" font="13">-0.033475146593</text>
<text top="374" left="459" width="117" height="10" font="13">Intercept.to.case</text>
<text top="374" left="644" width="96" height="10" font="13">0.722297491596</text>
<text top="390" left="459" width="117" height="10" font="13">1layhid.1.to.case</text>
<text top="390" left="637" width="103" height="10" font="13">-5.141324077052</text>
<text top="407" left="459" width="117" height="10" font="13">1layhid.2.to.case</text>
<text top="407" left="644" width="96" height="10" font="13">2.623245311046</text>
<text top="434" left="481" width="324" height="15" font="0">The training process needed 5254 steps until all</text>
<text top="452" left="459" width="346" height="15" font="0">absolute partial derivatives of the error function</text>
<text top="470" left="459" width="346" height="15" font="0">were smaller than 0.01 (the default threshold). The</text>
<text top="488" left="459" width="200" height="15" font="0">estimated weights range from</text>
<text top="487" left="663" width="12" height="15" font="6">−</text>
<text top="488" left="676" width="130" height="15" font="0">5.14 to 5.59. For in-</text>
<text top="506" left="459" width="346" height="15" font="0">stance, the intercepts of the first hidden layer are 5.59</text>
<text top="524" left="459" width="346" height="15" font="0">and 1.06 and the four weights leading to the first</text>
<text top="542" left="459" width="215" height="15" font="0">hidden neuron are estimated as</text>
<text top="541" left="679" width="12" height="15" font="6">−</text>
<text top="542" left="692" width="66" height="15" font="0">0.12, 1.77,</text>
<text top="541" left="763" width="12" height="15" font="6">−</text>
<text top="542" left="775" width="30" height="15" font="0">2.20,</text>
<text top="560" left="459" width="25" height="15" font="0">and</text>
<text top="559" left="491" width="12" height="15" font="6">−</text>
<text top="560" left="503" width="302" height="15" font="0">3.37 for the covariates age, parity, induced</text>
<text top="577" left="459" width="346" height="15" font="0">and spontaneous, respectively. If the error function</text>
<text top="595" left="459" width="346" height="15" font="0">is equal to the negative log-likelihood function, the</text>
<text top="613" left="459" width="346" height="15" font="0">error refers to the likelihood as is used for example</text>
<text top="631" left="459" width="328" height="15" font="0">to calculate Akaike’s Information Criterion (AIC).</text>
<text top="649" left="481" width="324" height="15" font="0">The given data is saved in nn$covariate and</text>
<text top="669" left="459" width="84" height="12" font="0">nn$response</text>
<text top="667" left="546" width="259" height="15" font="0">as well as in nn$data for the whole data</text>
<text top="685" left="459" width="346" height="15" font="0">set inclusive non-used variables. The output of the</text>
<text top="703" left="459" width="244" height="15" font="0">neural network, i.e. the fitted values o</text>
<text top="703" left="703" width="6" height="14" font="6">(</text>
<text top="703" left="710" width="7" height="15" font="3"><b>x</b></text>
<text top="703" left="718" width="6" height="14" font="6">)</text>
<text top="703" left="724" width="81" height="15" font="0">, is provided</text>
<text top="721" left="459" width="123" height="15" font="0">by nn$net.result:</text>
<text top="751" left="459" width="192" height="10" font="13">&gt; out &lt;- cbind(nn$covariate,</text>
<text top="768" left="459" width="7" height="10" font="13">+</text>
<text top="768" left="562" width="130" height="10" font="13">nn$net.result[[1]])</text>
<text top="784" left="459" width="199" height="10" font="13">&gt; dimnames(out) &lt;- list(NULL,</text>
<text top="801" left="459" width="7" height="10" font="13">+</text>
<text top="801" left="589" width="185" height="10" font="13">c(&#34;age&#34;,&#34;parity&#34;,&#34;induced&#34;,</text>
<text top="817" left="459" width="7" height="10" font="13">+</text>
<text top="817" left="603" width="185" height="10" font="13">&#34;spontaneous&#34;,&#34;nn-output&#34;))</text>
<text top="833" left="459" width="75" height="10" font="13">&gt; head(out)</text>
<text top="850" left="493" width="206" height="10" font="13">age parity induced spontaneous</text>
<text top="850" left="726" width="62" height="10" font="13">nn-output</text>
<text top="866" left="459" width="27" height="10" font="13">[1,]</text>
<text top="866" left="500" width="14" height="10" font="13">26</text>
<text top="866" left="555" width="7" height="10" font="13">6</text>
<text top="866" left="610" width="7" height="10" font="13">1</text>
<text top="866" left="692" width="96" height="10" font="13">2 0.1519579877</text>
<text top="883" left="459" width="27" height="10" font="13">[2,]</text>
<text top="883" left="500" width="14" height="10" font="13">42</text>
<text top="883" left="555" width="7" height="10" font="13">1</text>
<text top="883" left="610" width="7" height="10" font="13">1</text>
<text top="883" left="692" width="96" height="10" font="13">0 0.6204480608</text>
<text top="899" left="459" width="27" height="10" font="13">[3,]</text>
<text top="899" left="500" width="14" height="10" font="13">39</text>
<text top="899" left="555" width="7" height="10" font="13">6</text>
<text top="899" left="610" width="7" height="10" font="13">2</text>
<text top="899" left="692" width="96" height="10" font="13">0 0.1428325816</text>
<text top="916" left="459" width="27" height="10" font="13">[4,]</text>
<text top="916" left="500" width="14" height="10" font="13">34</text>
<text top="916" left="555" width="7" height="10" font="13">4</text>
<text top="916" left="610" width="7" height="10" font="13">2</text>
<text top="916" left="692" width="96" height="10" font="13">0 0.1513351888</text>
<text top="932" left="459" width="27" height="10" font="13">[5,]</text>
<text top="932" left="500" width="14" height="10" font="13">35</text>
<text top="932" left="555" width="7" height="10" font="13">3</text>
<text top="932" left="610" width="7" height="10" font="13">1</text>
<text top="932" left="692" width="96" height="10" font="13">1 0.3516163154</text>
<text top="949" left="459" width="27" height="10" font="13">[6,]</text>
<text top="949" left="500" width="14" height="10" font="13">36</text>
<text top="949" left="555" width="7" height="10" font="13">4</text>
<text top="949" left="610" width="7" height="10" font="13">2</text>
<text top="949" left="692" width="96" height="10" font="13">1 0.4904344475</text>
<text top="976" left="481" width="324" height="15" font="0">In this case, the object nn$net.result is a list con-</text>
<text top="994" left="459" width="346" height="15" font="0">sisting of only one element relating to one calculated</text>
<text top="1012" left="459" width="346" height="15" font="0">replication. If more than one replication were calcu-</text>
<text top="1030" left="459" width="346" height="15" font="0">lated, the outputs would be saved each in a separate</text>
<text top="1048" left="459" width="346" height="15" font="0">list element. This approach is the same for all values</text>
<text top="1065" left="459" width="346" height="15" font="0">that change with replication apart from net.result</text>
<text top="1083" left="459" width="346" height="15" font="0">that is saved as matrix with one column for each</text>
<text top="1101" left="459" width="74" height="15" font="0">replication.</text>
<text top="1119" left="481" width="324" height="15" font="0">To compare the results, neural networks are</text>
<text top="1137" left="459" width="346" height="15" font="0">trained with the same parameter setting as above us-</text>
<text top="1155" left="459" width="346" height="15" font="0">ing <b>neuralnet </b>with algorithm=&#34;backprop&#34; and the</text>
<text top="1173" left="459" width="93" height="15" font="0">package <b>nnet</b>.</text>
<text top="1218" left="82" width="220" height="15" font="0">The R Journal Vol. 2/1, June 2010</text>
<text top="1218" left="704" width="102" height="15" font="0">ISSN 2073-4859</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="1262" width="892">
<text top="67" left="83" width="11" height="15" font="0">C</text>
<text top="69" left="94" width="86" height="12" font="1">ONTRIBUTED</text>
<text top="67" left="184" width="10" height="15" font="0">R</text>
<text top="69" left="195" width="61" height="12" font="1">ESEARCH</text>
<text top="67" left="261" width="12" height="15" font="0">A</text>
<text top="69" left="273" width="53" height="12" font="1">RTICLES</text>
<text top="67" left="790" width="15" height="15" font="0">35</text>
<text top="127" left="82" width="144" height="10" font="13">&gt; nn.bp &lt;- neuralnet(</text>
<text top="144" left="82" width="7" height="10" font="13">+</text>
<text top="144" left="158" width="247" height="10" font="13">case~age+parity+induced+spontaneous,</text>
<text top="160" left="82" width="7" height="10" font="13">+</text>
<text top="160" left="158" width="247" height="10" font="13">data=infert, hidden=2, err.fct=&#34;ce&#34;,</text>
<text top="177" left="82" width="7" height="10" font="13">+</text>
<text top="177" left="158" width="137" height="10" font="13">linear.output=FALSE,</text>
<text top="193" left="82" width="7" height="10" font="13">+</text>
<text top="193" left="158" width="144" height="10" font="13">algorithm=&#34;backprop&#34;,</text>
<text top="210" left="82" width="7" height="10" font="13">+</text>
<text top="210" left="158" width="123" height="10" font="13">learningrate=0.01)</text>
<text top="226" left="82" width="48" height="10" font="13">&gt; nn.bp</text>
<text top="242" left="82" width="34" height="10" font="13">Call:</text>
<text top="259" left="96" width="69" height="10" font="13">neuralnet(</text>
<text top="275" left="96" width="316" height="10" font="13">formula = case~age+parity+induced+spontaneous,</text>
<text top="292" left="96" width="322" height="10" font="13">data = infert, hidden = 2, learningrate = 0.01,</text>
<text top="308" left="96" width="268" height="10" font="13">algorithm = &#34;backprop&#34;, err.fct = &#34;ce&#34;,</text>
<text top="325" left="96" width="151" height="10" font="13">linear.output = FALSE)</text>
<text top="357" left="82" width="192" height="10" font="13">1 repetition was calculated.</text>
<text top="390" left="131" width="199" height="10" font="13">Error Reached Threshold Steps</text>
<text top="407" left="82" width="82" height="10" font="13">1 158.085556</text>
<text top="407" left="192" width="96" height="10" font="13">0.008087314995</text>
<text top="407" left="323" width="7" height="10" font="13">4</text>
<text top="423" left="82" width="7" height="10" font="13">&gt;</text>
<text top="440" left="82" width="7" height="10" font="13">&gt;</text>
<text top="456" left="82" width="123" height="10" font="13">&gt; nn.nnet &lt;- nnet(</text>
<text top="473" left="82" width="7" height="10" font="13">+</text>
<text top="473" left="172" width="247" height="10" font="13">case~age+parity+induced+spontaneous,</text>
<text top="489" left="82" width="7" height="10" font="13">+</text>
<text top="489" left="172" width="213" height="10" font="13">data=infert, size=2, entropy=T,</text>
<text top="505" left="82" width="7" height="10" font="13">+</text>
<text top="505" left="172" width="82" height="10" font="13">abstol=0.01)</text>
<text top="522" left="82" width="69" height="10" font="13"># weights:</text>
<text top="522" left="165" width="14" height="10" font="13">13</text>
<text top="538" left="82" width="48" height="10" font="13">initial</text>
<text top="538" left="144" width="110" height="10" font="13">value 158.121035</text>
<text top="555" left="82" width="34" height="10" font="13">final</text>
<text top="555" left="131" width="110" height="10" font="13">value 158.085463</text>
<text top="571" left="82" width="62" height="10" font="13">converged</text>
<text top="598" left="105" width="38" height="12" font="0">nn.bp</text>
<text top="597" left="151" width="230" height="15" font="0">and nn.nnet show equal results.</text>
<text top="597" left="398" width="31" height="15" font="0">Both</text>
<text top="615" left="82" width="346" height="15" font="0">training processes last only a very few iteration steps</text>
<text top="633" left="82" width="346" height="15" font="0">and the error is approximately 158. Thus in this little</text>
<text top="651" left="82" width="346" height="15" font="0">comparison, the model fit is less satisfying than that</text>
<text top="669" left="82" width="255" height="15" font="0">achieved by resilient backpropagation.</text>
<text top="687" left="105" width="66" height="15" font="3"><b>neuralnet</b></text>
<text top="686" left="174" width="255" height="15" font="0">includes the calculation of generalized</text>
<text top="704" left="82" width="164" height="15" font="0">weights as introduced by</text>
<text top="704" left="250" width="171" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Intrator and Intrator (2001</a></text>
<text top="704" left="420" width="9" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">).</a></text>
<text top="722" left="82" width="168" height="15" font="0">The generalized weight ˜</text>
<text top="722" left="241" width="11" height="15" font="0">w</text>
<text top="728" left="252" width="3" height="11" font="1">i</text>
<text top="722" left="259" width="170" height="15" font="0">is defined as the contribu-</text>
<text top="740" left="82" width="261" height="15" font="0">tion of the ith covariate to the log-odds:</text>
<text top="788" left="195" width="5" height="15" font="0">˜</text>
<text top="789" left="190" width="11" height="15" font="0">w</text>
<text top="794" left="201" width="3" height="11" font="1">i</text>
<text top="789" left="209" width="12" height="14" font="6">=</text>
<text top="777" left="226" width="7" height="11" font="12"><i>∂</i></text>
<text top="773" left="235" width="21" height="15" font="0">log</text>
<text top="767" left="258" width="9" height="7" font="0"></text>
<text top="768" left="277" width="21" height="11" font="1">o(<b>x</b>)</text>
<text top="782" left="269" width="36" height="11" font="1">1−o(<b>x</b>)</text>
<text top="767" left="307" width="9" height="7" font="0"></text>
<text top="802" left="261" width="15" height="11" font="12"><i>∂</i>x</text>
<text top="804" left="277" width="3" height="11" font="1">i</text>
<text top="789" left="318" width="4" height="15" font="0">.</text>
<text top="824" left="82" width="346" height="15" font="0">The generalized weight expresses the effect of each</text>
<text top="842" left="82" width="71" height="15" font="0">covariate x</text>
<text top="848" left="154" width="3" height="11" font="1">i</text>
<text top="842" left="161" width="268" height="15" font="0">and thus has an analogous interpretation</text>
<text top="860" left="82" width="346" height="15" font="0">as the ith regression parameter in regression mod-</text>
<text top="878" left="82" width="346" height="15" font="0">els. However, the generalized weight depends on all</text>
<text top="896" left="82" width="346" height="15" font="0">other covariates. Its distribution indicates whether</text>
<text top="914" left="82" width="346" height="15" font="0">the effect of the covariate is linear since a small vari-</text>
<text top="932" left="82" width="204" height="15" font="0">ance suggests a linear effect <a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">(</a></text>
<text top="932" left="287" width="142" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">Intrator and Intrator,</a></text>
<text top="950" left="82" width="30" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">2001</a></text>
<text top="950" left="112" width="317" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#8">). </a>They are saved in nn$generalized.weights</text>
<text top="968" left="82" width="346" height="15" font="0">and are given in the following format (rounded val-</text>
<text top="986" left="82" width="27" height="15" font="0">ues)</text>
<text top="1014" left="82" width="240" height="10" font="13">&gt; head(nn$generalized.weights[[1]])</text>
<text top="1031" left="131" width="27" height="10" font="13">[,1]</text>
<text top="1031" left="206" width="27" height="10" font="13">[,2]</text>
<text top="1031" left="275" width="27" height="10" font="13">[,3]</text>
<text top="1031" left="343" width="27" height="10" font="13">[,4]</text>
<text top="1047" left="82" width="288" height="10" font="13">1 0.0088556 -0.1330079 0.1657087 0.2537842</text>
<text top="1064" left="82" width="288" height="10" font="13">2 0.1492874 -2.2422321 2.7934978 4.2782645</text>
<text top="1080" left="82" width="288" height="10" font="13">3 0.0004489 -0.0067430 0.0084008 0.0128660</text>
<text top="1097" left="82" width="288" height="10" font="13">4 0.0083028 -0.1247051 0.1553646 0.2379421</text>
<text top="1113" left="82" width="288" height="10" font="13">5 0.1071413 -1.6092161 2.0048511 3.0704457</text>
<text top="1129" left="82" width="288" height="10" font="13">6 0.1360035 -2.0427123 2.5449249 3.8975730</text>
<text top="1155" left="105" width="308" height="15" font="0">The columns refer to the four covariates age (j</text>
<text top="1155" left="417" width="12" height="14" font="6">=</text>
<text top="1173" left="82" width="81" height="15" font="0">1), parity (j</text>
<text top="1173" left="168" width="12" height="14" font="6">=</text>
<text top="1173" left="184" width="89" height="15" font="0">2), induced (j</text>
<text top="1173" left="278" width="12" height="14" font="6">=</text>
<text top="1173" left="294" width="135" height="15" font="0">3), and spontaneous</text>
<text top="125" left="459" width="10" height="15" font="0">(j</text>
<text top="125" left="472" width="12" height="14" font="6">=</text>
<text top="125" left="488" width="318" height="15" font="0">4) and a generalized weight is given for each ob-</text>
<text top="143" left="459" width="346" height="15" font="0">servation even though they are equal for each covari-</text>
<text top="161" left="459" width="110" height="15" font="0">ate combination.</text>
<text top="202" left="459" width="184" height="17" font="11"><b>Visualizing the results</b></text>
<text top="233" left="459" width="346" height="15" font="0">The results of the training process can be visualized</text>
<text top="251" left="459" width="346" height="15" font="0">by two different plots. First, the trained neural net-</text>
<text top="269" left="459" width="201" height="15" font="0">work can simply be plotted by</text>
<text top="301" left="459" width="69" height="10" font="13">&gt; plot(nn)</text>
<text top="331" left="481" width="236" height="15" font="0">The resulting plot is given in Figure</text>
<text top="331" left="721" width="7" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#6">3</a></text>
<text top="331" left="728" width="4" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#6">.</a></text>
<text top="757" left="459" width="346" height="15" font="0">Figure 3: Plot of a trained neural network includ-</text>
<text top="775" left="459" width="346" height="15" font="0">ing trained synaptic weights and basic information</text>
<text top="793" left="459" width="175" height="15" font="0">about the training process.</text>
<text top="827" left="481" width="324" height="15" font="0">It reflects the structure of the trained neural net-</text>
<text top="845" left="459" width="346" height="15" font="0">work, i.e. the network topology. The plot includes</text>
<text top="863" left="459" width="346" height="15" font="0">by default the trained synaptic weights, all intercepts</text>
<text top="881" left="459" width="346" height="15" font="0">as well as basic information about the training pro-</text>
<text top="899" left="459" width="346" height="15" font="0">cess like the overall error and the number of steps</text>
<text top="917" left="459" width="346" height="15" font="0">needed to converge. Especially for larger neural net-</text>
<text top="935" left="459" width="346" height="15" font="0">works, the size of the plot and that of each neuron</text>
<text top="953" left="459" width="346" height="15" font="0">can be determined using the parameters dimension</text>
<text top="971" left="459" width="164" height="15" font="0">and radius, respectively.</text>
<text top="989" left="481" width="324" height="15" font="0">The second possibility to visualize the results</text>
<text top="1007" left="459" width="231" height="15" font="0">is to plot generalized weights.</text>
<text top="1008" left="719" width="46" height="12" font="0">gwplot</text>
<text top="1007" left="776" width="29" height="15" font="0">uses</text>
<text top="1025" left="459" width="346" height="15" font="0">the calculated generalized weights provided by</text>
<text top="1044" left="459" width="168" height="12" font="0">nn$generalized.weights</text>
<text top="1043" left="630" width="175" height="15" font="0">and can be used by the fol-</text>
<text top="1061" left="459" width="125" height="15" font="0">lowing statements:</text>
<text top="1093" left="459" width="130" height="10" font="13">&gt; par(mfrow=c(2,2))</text>
<text top="1110" left="459" width="254" height="10" font="13">&gt; gwplot(nn,selected.covariate=&#34;age&#34;,</text>
<text top="1126" left="459" width="7" height="10" font="13">+</text>
<text top="1126" left="521" width="110" height="10" font="13">min=-2.5, max=5)</text>
<text top="1142" left="459" width="274" height="10" font="13">&gt; gwplot(nn,selected.covariate=&#34;parity&#34;,</text>
<text top="1159" left="459" width="7" height="10" font="13">+</text>
<text top="1159" left="521" width="110" height="10" font="13">min=-2.5, max=5)</text>
<text top="1175" left="459" width="281" height="10" font="13">&gt; gwplot(nn,selected.covariate=&#34;induced&#34;,</text>
<text top="1218" left="82" width="220" height="15" font="0">The R Journal Vol. 2/1, June 2010</text>
<text top="1218" left="704" width="102" height="15" font="0">ISSN 2073-4859</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="1262" width="892">
<text top="67" left="82" width="15" height="15" font="0">36</text>
<text top="67" left="562" width="11" height="15" font="0">C</text>
<text top="69" left="573" width="86" height="12" font="1">ONTRIBUTED</text>
<text top="67" left="664" width="10" height="15" font="0">R</text>
<text top="69" left="674" width="61" height="12" font="1">ESEARCH</text>
<text top="67" left="740" width="12" height="15" font="0">A</text>
<text top="69" left="752" width="53" height="12" font="1">RTICLES</text>
<text top="127" left="82" width="7" height="10" font="13">+</text>
<text top="127" left="144" width="110" height="10" font="13">min=-2.5, max=5)</text>
<text top="144" left="82" width="309" height="10" font="13">&gt; gwplot(nn,selected.covariate=&#34;spontaneous&#34;,</text>
<text top="160" left="82" width="7" height="10" font="13">+</text>
<text top="160" left="144" width="110" height="10" font="13">min=-2.5, max=5)</text>
<text top="192" left="105" width="281" height="15" font="0">The corresponding plot is shown in Figure</text>
<text top="192" left="389" width="7" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#7">4</a></text>
<text top="192" left="397" width="4" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#7">.</a></text>
<text top="661" left="82" width="346" height="15" font="0">Figure 4: Plots of generalized weights with respect</text>
<text top="679" left="82" width="115" height="15" font="0">to each covariate.</text>
<text top="710" left="105" width="324" height="15" font="0">The generalized weights are given for all covari-</text>
<text top="728" left="82" width="346" height="15" font="0">ates within the same range. The distribution of the</text>
<text top="746" left="82" width="346" height="15" font="0">generalized weights suggests that the covariate age</text>
<text top="764" left="82" width="346" height="15" font="0">has no effect on the case-control status since all gen-</text>
<text top="782" left="82" width="346" height="15" font="0">eralized weights are nearly zero and that at least the</text>
<text top="800" left="82" width="346" height="15" font="0">two covariates induced and spontaneous have a non-</text>
<text top="818" left="82" width="346" height="15" font="0">linear effect since the variance of their generalized</text>
<text top="836" left="82" width="231" height="15" font="0">weights is overall greater than one.</text>
<text top="880" left="82" width="160" height="17" font="11"><b>Additional features</b></text>
<text top="911" left="82" width="145" height="15" font="3"><b>The </b>compute <b>function</b></text>
<text top="941" left="82" width="53" height="12" font="0">compute</text>
<text top="940" left="143" width="286" height="15" font="0">calculates and summarizes the output of</text>
<text top="958" left="82" width="346" height="15" font="0">each neuron, i.e. all neurons in the input, hidden and</text>
<text top="976" left="82" width="346" height="15" font="0">output layer. Thus, it can be used to trace all sig-</text>
<text top="994" left="82" width="346" height="15" font="0">nals passing the neural network for given covariate</text>
<text top="1012" left="82" width="346" height="15" font="0">combinations. This helps to interpret the network</text>
<text top="1030" left="82" width="346" height="15" font="0">topology of a trained neural network. It can also eas-</text>
<text top="1048" left="82" width="346" height="15" font="0">ily be used to calculate predictions for new covari-</text>
<text top="1065" left="82" width="346" height="15" font="0">ate combinations. A neural network is trained with</text>
<text top="1083" left="82" width="346" height="15" font="0">a training data set consisting of known input-output</text>
<text top="1101" left="82" width="346" height="15" font="0">pairs. It learns an approximation of the relationship</text>
<text top="1119" left="82" width="346" height="15" font="0">between inputs and outputs and can then be used</text>
<text top="1137" left="82" width="133" height="15" font="0">to predict outputs o</text>
<text top="1137" left="216" width="6" height="14" font="6">(</text>
<text top="1137" left="222" width="7" height="15" font="3"><b>x</b></text>
<text top="1142" left="230" width="20" height="11" font="1">new</text>
<text top="1137" left="250" width="6" height="14" font="6">)</text>
<text top="1137" left="262" width="167" height="15" font="0">relating to new covariate</text>
<text top="1155" left="82" width="102" height="15" font="0">combinations <b>x</b></text>
<text top="1160" left="185" width="20" height="11" font="1">new</text>
<text top="1155" left="205" width="224" height="15" font="0">. The function compute simplifies</text>
<text top="1173" left="82" width="346" height="15" font="0">this calculation. It automatically redefines the struc-</text>
<text top="125" left="459" width="346" height="15" font="0">ture of the given neural network and calculates the</text>
<text top="143" left="459" width="291" height="15" font="0">output for arbitrary covariate combinations.</text>
<text top="161" left="481" width="324" height="15" font="0">To stay with the example, predicted outputs</text>
<text top="179" left="459" width="346" height="15" font="0">can be calculated for instance for missing com-</text>
<text top="197" left="459" width="307" height="15" font="0">binations with age=22, parity=1, induced</text>
<text top="196" left="774" width="12" height="15" font="6">≤</text>
<text top="198" left="794" width="8" height="12" font="0">1</text>
<text top="197" left="802" width="4" height="15" font="0">,</text>
<text top="215" left="459" width="120" height="15" font="0">and spontaneous</text>
<text top="214" left="586" width="12" height="15" font="6">≤</text>
<text top="216" left="606" width="8" height="12" font="0">1</text>
<text top="215" left="614" width="4" height="15" font="0">.</text>
<text top="215" left="643" width="163" height="15" font="0">They are provided by</text>
<text top="234" left="459" width="160" height="12" font="0">new.output$net.result</text>
<text top="265" left="459" width="185" height="10" font="13">&gt; new.output &lt;- compute(nn,</text>
<text top="281" left="569" width="192" height="10" font="13">covariate=matrix(c(22,1,0,0,</text>
<text top="298" left="699" width="62" height="10" font="13">22,1,1,0,</text>
<text top="314" left="699" width="62" height="10" font="13">22,1,0,1,</text>
<text top="331" left="699" width="69" height="10" font="13">22,1,1,1),</text>
<text top="347" left="637" width="137" height="10" font="13">byrow=TRUE, ncol=4))</text>
<text top="364" left="459" width="158" height="10" font="13">&gt; new.output$net.result</text>
<text top="380" left="527" width="27" height="10" font="13">[,1]</text>
<text top="396" left="459" width="96" height="10" font="13">[1,] 0.1477097</text>
<text top="413" left="459" width="96" height="10" font="13">[2,] 0.1929026</text>
<text top="429" left="459" width="96" height="10" font="13">[3,] 0.3139651</text>
<text top="446" left="459" width="96" height="10" font="13">[4,] 0.8516760</text>
<text top="475" left="481" width="324" height="15" font="0">This means that the predicted probability of being</text>
<text top="493" left="459" width="346" height="15" font="0">a case given the mentioned covariate combinations,</text>
<text top="511" left="459" width="30" height="15" font="0">i.e. o</text>
<text top="511" left="489" width="6" height="14" font="6">(</text>
<text top="511" left="496" width="7" height="15" font="3"><b>x</b></text>
<text top="511" left="504" width="6" height="14" font="6">)</text>
<text top="511" left="510" width="296" height="15" font="0">, is increasing in this example with the num-</text>
<text top="529" left="459" width="145" height="15" font="0">ber of prior abortions.</text>
<text top="569" left="459" width="237" height="15" font="3"><b>The </b>confidence.interval <b>function</b></text>
<text top="598" left="459" width="346" height="15" font="0">The weights of a neural network follow a multivari-</text>
<text top="616" left="459" width="346" height="15" font="0">ate normal distribution if the network is identified</text>
<text top="633" left="459" width="5" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">(</a></text>
<text top="633" left="464" width="80" height="15" font="5"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">White, 1989</a></text>
<text top="633" left="544" width="262" height="15" font="0"><a href="2010-1-RJournal_2010-1_Guenther+Fritsch.html#9">). </a>A neural network is identified if it</text>
<text top="651" left="459" width="346" height="15" font="0">does not include irrelevant neurons neither in the</text>
<text top="669" left="459" width="346" height="15" font="0">input layer nor in the hidden layers. An irrelevant</text>
<text top="687" left="459" width="346" height="15" font="0">neuron in the input layer can be for instance a co-</text>
<text top="705" left="459" width="346" height="15" font="0">variate that has no effect or that is a linear combi-</text>
<text top="723" left="459" width="346" height="15" font="0">nation of other included covariates. If this restric-</text>
<text top="741" left="459" width="346" height="15" font="0">tion is fulfilled and if the error function equals the</text>
<text top="759" left="459" width="346" height="15" font="0">neagtive log-likelihood, a confidence interval can be</text>
<text top="777" left="459" width="346" height="15" font="0">calculated for each weight. The <b>neuralnet </b>package</text>
<text top="795" left="459" width="346" height="15" font="0">provides a function to calculate these confidence in-</text>
<text top="813" left="459" width="346" height="15" font="0">tervals regardless of whether all restrictions are ful-</text>
<text top="831" left="459" width="346" height="15" font="0">filled. Therefore, the user has to be careful interpret-</text>
<text top="849" left="459" width="97" height="15" font="0">ing the results.</text>
<text top="867" left="481" width="324" height="15" font="0">Since the covariate age has no effect on the out-</text>
<text top="885" left="459" width="346" height="15" font="0">come and the related neuron is thus irrelevant, a new</text>
<text top="902" left="459" width="346" height="15" font="0">neural network (nn.new), which has only the three</text>
<text top="920" left="459" width="346" height="15" font="0">input variables parity, induced, and spontaneous,</text>
<text top="938" left="459" width="346" height="15" font="0">has to be trained to demonstrate the usage of</text>
<text top="958" left="459" width="145" height="12" font="0">confidence.interval</text>
<text top="956" left="604" width="202" height="15" font="0">. Let us assume that all restric-</text>
<text top="974" left="459" width="346" height="15" font="0">tions are now fulfilled, i.e. neither the three input</text>
<text top="992" left="459" width="346" height="15" font="0">variables nor the two hidden neurons are irrelevant.</text>
<text top="1010" left="459" width="346" height="15" font="0">Confidence intervals can then be calculated with the</text>
<text top="1028" left="459" width="208" height="15" font="0">function confidence.interval:</text>
<text top="1060" left="459" width="322" height="10" font="13">&gt; ci &lt;- confidence.interval(nn.new, alpha=0.05)</text>
<text top="1077" left="459" width="89" height="10" font="13">&gt; ci$lower.ci</text>
<text top="1093" left="459" width="34" height="10" font="13">[[1]]</text>
<text top="1110" left="459" width="69" height="10" font="13">[[1]][[1]]</text>
<text top="1126" left="555" width="27" height="10" font="13">[,1]</text>
<text top="1126" left="651" width="27" height="10" font="13">[,2]</text>
<text top="1142" left="459" width="27" height="10" font="13">[1,]</text>
<text top="1142" left="507" width="75" height="10" font="13">1.830803796</text>
<text top="1142" left="596" width="82" height="10" font="13">-2.680895286</text>
<text top="1159" left="459" width="27" height="10" font="13">[2,]</text>
<text top="1159" left="507" width="75" height="10" font="13">1.673863304</text>
<text top="1159" left="596" width="82" height="10" font="13">-2.839908343</text>
<text top="1175" left="459" width="27" height="10" font="13">[3,]</text>
<text top="1175" left="500" width="178" height="10" font="13">-8.883004913 -37.232020925</text>
<text top="1218" left="82" width="220" height="15" font="0">The R Journal Vol. 2/1, June 2010</text>
<text top="1218" left="704" width="102" height="15" font="0">ISSN 2073-4859</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="1262" width="892">
<text top="67" left="83" width="11" height="15" font="0">C</text>
<text top="69" left="94" width="86" height="12" font="1">ONTRIBUTED</text>
<text top="67" left="184" width="10" height="15" font="0">R</text>
<text top="69" left="195" width="61" height="12" font="1">ESEARCH</text>
<text top="67" left="261" width="12" height="15" font="0">A</text>
<text top="69" left="273" width="53" height="12" font="1">RTICLES</text>
<text top="67" left="790" width="15" height="15" font="0">37</text>
<text top="127" left="82" width="219" height="10" font="13">[4,] -48.906348154 -18.748849335</text>
<text top="160" left="82" width="69" height="10" font="13">[[1]][[2]]</text>
<text top="177" left="172" width="27" height="10" font="13">[,1]</text>
<text top="193" left="82" width="27" height="10" font="13">[1,]</text>
<text top="193" left="124" width="75" height="10" font="13">1.283391149</text>
<text top="210" left="82" width="117" height="10" font="13">[2,] -3.724315385</text>
<text top="226" left="82" width="117" height="10" font="13">[3,] -2.650545922</text>
<text top="262" left="105" width="324" height="15" font="0">For each weight, ci$lower.ci provides the re-</text>
<text top="279" left="82" width="346" height="15" font="0">lated lower confidence limit and ci$upper.ci the re-</text>
<text top="297" left="82" width="346" height="15" font="0">lated upper confidence limit. The first matrix con-</text>
<text top="315" left="82" width="346" height="15" font="0">tains the limits of the weights leading to the hidden</text>
<text top="333" left="82" width="346" height="15" font="0">neurons. The columns refer to the two hidden neu-</text>
<text top="351" left="82" width="346" height="15" font="0">rons. The other three values are the limits of the</text>
<text top="369" left="82" width="250" height="15" font="0">weights leading to the output neuron.</text>
<text top="423" left="82" width="96" height="21" font="4"><b>Summary</b></text>
<text top="464" left="82" width="346" height="15" font="0">This paper gave a brief introduction to multi-layer</text>
<text top="482" left="82" width="346" height="15" font="0">perceptrons and supervised learning algorithms. It</text>
<text top="500" left="82" width="346" height="15" font="0">introduced the package <b>neuralnet </b>that can be ap-</text>
<text top="518" left="82" width="346" height="15" font="0">plied when modeling functional relationships be-</text>
<text top="536" left="82" width="346" height="15" font="0">tween covariates and response variables. <b>neuralnet</b></text>
<text top="554" left="82" width="346" height="15" font="0">contains a very flexible function that trains multi-</text>
<text top="572" left="82" width="346" height="15" font="0">layer perceptrons to a given data set in the context</text>
<text top="590" left="82" width="346" height="15" font="0">of regression analyses. It is a very flexible package</text>
<text top="608" left="82" width="346" height="15" font="0">since most parameters can be easily adapted. For ex-</text>
<text top="625" left="82" width="346" height="15" font="0">ample, the activation function and the error function</text>
<text top="643" left="82" width="346" height="15" font="0">can be arbitrarily chosen and can be defined by the</text>
<text top="661" left="82" width="221" height="15" font="0">usual definition of functions in R.</text>
<text top="715" left="82" width="196" height="21" font="4"><b>Acknowledgements</b></text>
<text top="756" left="82" width="346" height="15" font="0">The authors thank Nina Wawro for reading prelim-</text>
<text top="774" left="82" width="346" height="15" font="0">inary versions of the paper and for giving helpful</text>
<text top="792" left="82" width="346" height="15" font="0">comments. Additionally, we would like to thank two</text>
<text top="810" left="82" width="346" height="15" font="0">anonymous reviewers for their valuable suggestions</text>
<text top="828" left="82" width="87" height="15" font="0">and remarks.</text>
<text top="847" left="105" width="324" height="15" font="0">We gratefully acknowledge the financial support</text>
<text top="865" left="82" width="346" height="15" font="0">of this research by the grant PI 345/3-1 from the Ger-</text>
<text top="883" left="82" width="221" height="15" font="0">man Research Foundation (DFG).</text>
<text top="937" left="82" width="130" height="21" font="4"><b>Bibliography</b></text>
<text top="977" left="82" width="74" height="15" font="0">H. Akaike.</text>
<text top="977" left="173" width="256" height="15" font="0">Information theory and an extension</text>
<text top="995" left="97" width="332" height="15" font="0">of the maximum likelihood principle. In Petrov</text>
<text top="1013" left="97" width="332" height="15" font="0">BN and Csaki BF, editors, Second international</text>
<text top="1031" left="97" width="332" height="15" font="0">symposium on information theory, pages 267–281.</text>
<text top="1049" left="97" width="226" height="15" font="0">Academiai Kiado, Budapest, 1973.</text>
<text top="1083" left="82" width="346" height="15" font="0">C. Almeida, C. Baugh, C. Lacey, C. Frenk, G. Granato,</text>
<text top="1101" left="97" width="332" height="15" font="0">L. Silva, and A. Bressan. Modelling the dsty uni-</text>
<text top="1119" left="97" width="332" height="15" font="0">verse i: Introducing the artificial neural network</text>
<text top="1137" left="97" width="332" height="15" font="0">and first applications to luminosity and colour dis-</text>
<text top="1155" left="97" width="332" height="15" font="0">tributions. Monthly Notices of the Royal Astronomical</text>
<text top="1173" left="97" width="170" height="15" font="0">Society, 402:544–564, 2010.</text>
<text top="125" left="459" width="346" height="15" font="0">A. Anastasiadis, G. Magoulas, and M. Vrahatis. New</text>
<text top="143" left="474" width="332" height="15" font="0">globally convergent training scheme based on the</text>
<text top="161" left="474" width="332" height="15" font="0">resilient propagation algorithm. Neurocomputing,</text>
<text top="179" left="474" width="112" height="15" font="0">64:253–270, 2005.</text>
<text top="209" left="459" width="346" height="15" font="0">C. Bishop. Neural networks for pattern recognition. Ox-</text>
<text top="227" left="474" width="255" height="15" font="0">ford University Press, New York, 1995.</text>
<text top="257" left="459" width="346" height="15" font="0">S. Fritsch and F. Günther. neuralnet: Training of Neural</text>
<text top="275" left="474" width="332" height="15" font="0">Networks. R Foundation for Statistical Computing,</text>
<text top="293" left="474" width="186" height="15" font="0">2008. R package version 1.2.</text>
<text top="323" left="459" width="346" height="15" font="0">F. Günther, N. Wawro, and K. Bammann. Neural net-</text>
<text top="341" left="474" width="332" height="15" font="0">works for modeling gene-gene interactions in as-</text>
<text top="359" left="474" width="289" height="15" font="0">sociation studies. BMC Genetics, 10:87, 2009.</text>
<text top="360" left="767" width="38" height="12" font="5"><a href="http://www.biomedcentral.com/1471-2156/10/87">http:</a></text>
<text top="378" left="474" width="298" height="12" font="5"><a href="http://www.biomedcentral.com/1471-2156/10/87">//www.biomedcentral.com/1471-2156/10/87</a></text>
<text top="377" left="772" width="4" height="15" font="0"><a href="http://www.biomedcentral.com/1471-2156/10/87">.</a></text>
<text top="407" left="459" width="346" height="15" font="0">K. Hornik, M. Stichcombe, and H. White. Multi-</text>
<text top="424" left="474" width="332" height="15" font="0">layer feedforward networks are universal approx-</text>
<text top="442" left="474" width="276" height="15" font="0">imators. Neural Networks, 2:359–366, 1989.</text>
<text top="472" left="459" width="193" height="15" font="0">O. Intrator and N. Intrator.</text>
<text top="472" left="670" width="135" height="15" font="0">Interpreting neural-</text>
<text top="490" left="474" width="332" height="15" font="0">network results: a simulation study. Computational</text>
<text top="508" left="474" width="281" height="15" font="0">Statistics &amp; Data Analysis, 37:373–393, 2001.</text>
<text top="538" left="459" width="346" height="15" font="0">A. Kumar and D. Zhang. Personal recognition using</text>
<text top="556" left="474" width="332" height="15" font="0">hand shape and texture. IEEE Transactions on Image</text>
<text top="574" left="474" width="199" height="15" font="0">Processing, 15:2454–2461, 2006.</text>
<text top="604" left="459" width="346" height="15" font="0">M. C. Limas, E. P. V. G. Joaquín B. Ordieres Meré,</text>
<text top="622" left="474" width="332" height="15" font="0">F. J. M. de Pisón Ascacibar, A. V. P. Espinoza, and</text>
<text top="640" left="474" width="332" height="15" font="0">F. A. Elías. AMORE: A MORE Flexible Neural Net-</text>
<text top="658" left="474" width="159" height="15" font="0">work Package, 2007. URL</text>
<text top="659" left="637" width="168" height="12" font="5"><a href="http://wiki.r-project.org/rwiki/doku.php?id=packages:cran:amore">http://wiki.r-project.</a></text>
<text top="677" left="474" width="312" height="12" font="5"><a href="http://wiki.r-project.org/rwiki/doku.php?id=packages:cran:amore">org/rwiki/doku.php?id=packages:cran:amore</a></text>
<text top="676" left="786" width="19" height="15" font="0"><a href="http://wiki.r-project.org/rwiki/doku.php?id=packages:cran:amore">. </a>R</text>
<text top="694" left="474" width="153" height="15" font="0">package version 0.2-11.</text>
<text top="724" left="459" width="346" height="15" font="0">P. McCullagh and J. Nelder. Generalized Linear Models.</text>
<text top="742" left="474" width="227" height="15" font="0">Chapman and Hall, London, 1983.</text>
<text top="772" left="459" width="98" height="15" font="0">M. Riedmiller.</text>
<text top="772" left="574" width="231" height="15" font="0">Advanced supervised learning in</text>
<text top="790" left="474" width="332" height="15" font="0">multi-layer perceptrons - from backpropagation to</text>
<text top="808" left="474" width="332" height="15" font="0">adaptive learning algorithms. International Jour-</text>
<text top="826" left="474" width="332" height="15" font="0">nal of Computer Standards and Interfaces, 16:265–278,</text>
<text top="844" left="474" width="34" height="15" font="0">1994.</text>
<text top="874" left="459" width="346" height="15" font="0">M. Riedmiller and H. Braun. A direct method for</text>
<text top="892" left="474" width="332" height="15" font="0">faster backpropagation learning: the rprop algo-</text>
<text top="910" left="474" width="332" height="15" font="0">rithm. Proceedings of the IEEE International Confer-</text>
<text top="927" left="474" width="317" height="15" font="0">ence on Neural Networks (ICNN), 1:586–591, 1993.</text>
<text top="957" left="459" width="346" height="15" font="0">M. Rocha, P. Cortez, and J. Neves. Evolutionary neu-</text>
<text top="975" left="474" width="332" height="15" font="0">ral network learning. Lecture Notes in Computer Sci-</text>
<text top="993" left="474" width="146" height="15" font="0">ence, 2902:24–28, 2003.</text>
<text top="1023" left="459" width="346" height="15" font="0">R. Rojas. Neural Networks. Springer-Verlag, Berlin,</text>
<text top="1041" left="474" width="34" height="15" font="0">1996.</text>
<text top="1071" left="459" width="346" height="15" font="0">W. Schiffmann, M. Joost, and R. Werner. Optimiza-</text>
<text top="1089" left="474" width="332" height="15" font="0">tion of the backpropagation algorithm for training</text>
<text top="1107" left="474" width="332" height="15" font="0">multilayer perceptrons. Technical report, Univer-</text>
<text top="1125" left="474" width="269" height="15" font="0">sity of Koblenz, Insitute of Physics, 1994.</text>
<text top="1155" left="459" width="346" height="15" font="0">G. Schwarz. Estimating the dimension of a model.</text>
<text top="1173" left="474" width="168" height="15" font="0">Ann Stat, 6:461–464, 1978.</text>
<text top="1218" left="82" width="220" height="15" font="0">The R Journal Vol. 2/1, June 2010</text>
<text top="1218" left="704" width="102" height="15" font="0">ISSN 2073-4859</text>
</page>
<page number="9" position="absolute" top="0" left="0" height="1262" width="892">
<text top="67" left="82" width="15" height="15" font="0">38</text>
<text top="67" left="562" width="11" height="15" font="0">C</text>
<text top="69" left="573" width="86" height="12" font="1">ONTRIBUTED</text>
<text top="67" left="664" width="10" height="15" font="0">R</text>
<text top="69" left="674" width="61" height="12" font="1">ESEARCH</text>
<text top="67" left="740" width="12" height="15" font="0">A</text>
<text top="69" left="752" width="53" height="12" font="1">RTICLES</text>
<text top="125" left="82" width="346" height="15" font="0">D. Trichopoulos, N. Handanos, J. Danezis, A. Kalan-</text>
<text top="143" left="97" width="332" height="15" font="0">didi, and V. Kalapothaki. Induced abortion and</text>
<text top="161" left="97" width="332" height="15" font="0">secondary infertility. British Journal of Obstetrics and</text>
<text top="179" left="97" width="197" height="15" font="0">Gynaecology, 83:645–650, 1976.</text>
<text top="222" left="82" width="346" height="15" font="0">W. Venables and B. Ripley. Modern Applied Statis-</text>
<text top="240" left="97" width="78" height="15" font="0">tics with S.</text>
<text top="240" left="197" width="232" height="15" font="0">Springer, New York, fourth edi-</text>
<text top="258" left="97" width="73" height="15" font="0">tion, 2002.</text>
<text top="258" left="192" width="31" height="15" font="0">URL</text>
<text top="259" left="231" width="198" height="12" font="5"><a href="http://www.stats.ox.ac.uk/pub/MASS4">http://www.stats.ox.ac.uk/</a></text>
<text top="277" left="97" width="69" height="12" font="5"><a href="http://www.stats.ox.ac.uk/pub/MASS4">pub/MASS4</a></text>
<text top="276" left="166" width="141" height="15" font="0"><a href="http://www.stats.ox.ac.uk/pub/MASS4">. </a>ISBN 0-387-95457-0.</text>
<text top="319" left="82" width="346" height="15" font="0">H. White. Learning in artificial neural networks: a</text>
<text top="125" left="474" width="332" height="15" font="0">statistical perspective. Neural Computation, 1:425–</text>
<text top="143" left="474" width="64" height="15" font="0">464, 1989.</text>
<text top="194" left="459" width="97" height="15" font="0">Frauke Günther</text>
<text top="212" left="459" width="346" height="15" font="0">University of Bremen, Bremen Institute for Prevention</text>
<text top="230" left="459" width="179" height="15" font="0">Research and Social Medicine</text>
<text top="249" left="459" width="206" height="12" font="5"><a href="mailto:guenther@bips.uni-bremen.de">guenther@bips.uni-bremen.de</a></text>
<text top="283" left="459" width="85" height="15" font="0">Stefan Fritsch</text>
<text top="301" left="459" width="346" height="15" font="0">University of Bremen, Bremen Institute for Prevention</text>
<text top="319" left="459" width="179" height="15" font="0">Research and Social Medicine</text>
<text top="1218" left="82" width="220" height="15" font="0">The R Journal Vol. 2/1, June 2010</text>
<text top="1218" left="704" width="102" height="15" font="0">ISSN 2073-4859</text>
</page>
</pdf2xml>
